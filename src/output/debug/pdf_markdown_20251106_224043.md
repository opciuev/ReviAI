
================================================================================
ドキュメント: プログラム基本設計書_累積作成ツール_AI審査_七回_V7.pdf (今回の設計書 V7)
ファイル名: プログラム基本設計書_累積作成ツール_AI審査_七回_V7.pdf
================================================================================

要求No 要求内容 (ペルソナ: 指令)

評価 (〇/△/×) 適合/不適合箇所

適合/不適合理由

修正案 (ゴールデンケースを含む)

SA-1

SystemArchitect: 論理トレーサビリティの検
証

△

設計書全体

SA-2

SystemArchitect: 機能要件と非機能要件の
分離と結合

△

「・リソース消費」、6.1、6.3.1

本設計書はツールの内部動作を詳細に記述しているが、どの機能がどのビジネス要件やルー
ルブックのどの項目に対応するかのトレーサビリティが欠落している。
例えば、「6.3.3 Parquetファイル出力共通設定」で言及される「io_div」や「compress_method」は
ルールブックを参照するよう記載があるが、設計書内で要件番号との直接的な紐付けがないた
め、設計の妥当性を要件から追跡することが困難である。

設計書の冒頭に「要件・設計対応表」を追加する。この表には、要件定義書やルールブック
の要求No、要求内容、および本設計書でその要求を実現している章番号や機能名をマッ
ピングする。例：| 要求No | 要求元 | 要求概要 | 対応設計箇所 | の形式で記述する。

性能やリソース消費に関する記述（RowGroupSize、メモリ500MBでのバッチ処理など）が、機能
仕様の記述内に散在している。
また、「あくまで、現状の構造で推測した数値です。テスト&運用上で修正する必要かもしれませ
ん」といった記述は、非機能要件に対する設計根拠の弱さを示している。非機能要件が体系的
に整理されておらず、パラメータの設計値の妥当性を評価できない。

独立した「4. 非機能要件」の章を設ける。そこで性能目標（例：100万件のデータをXX分以
内に処理）、リソース上限（例：メモリ使用量YYGB以内）を明記する。その上で、各パラメー
タ（RowGroupSize、バッチ処理の閾値等）が、なぜその非機能要件を満たすために妥当な
設定値と言えるのか、設計根拠を記述する。チューニングが必要な場合は、その指針や手
順も明記する。

QA-1

QAManager: 処理ロジックの具体性評価
(WDT分析)

×

6.2 コールバック駆動のデータ取込・正規化（to
Inputスキーマ）

「プラグインの対象データファイルを読み込みます」とあるが、各プラグイン（Excel, PDF, CSV等）
が具体的にどのようなロジックでデータを読み込み、DataRowに変換するかの仕様が全く記述さ
れていない。
「Do（処理）」が極めて曖昧であり、この記述ではテスト担当者は正常系・異常系のテストケース
を作成することが不可能である。

各プラグイン（少なくとも汎用プラグインであるTSV/CSV/固定長）について、独立した仕様
を記述する。 【ゴールデンケース（CSVプラグイン）】
1. **入力**: ファイルパス、エンコーディング、区切り文字、ヘッダー行の有無
2. **処理**:
   - 指定されたエンコーディングでファイルを開く。
   - ヘッダー行がある場合、1行目を読み飛ばす。
   - 1行ずつ読み込み、指定された区切り文字で分割する。
   - 分割後の各要素をInputスキーマの対応する列にマッピングする。
3. **出力**: onDataRowデリゲート経由でDataRowを返却。
4. **異常系**:
   - ファイルが見つからない場合のエラーコード。
   - 列数がスキーマと一致しない場合の処理（エラー、スキップなど）。

QA-2

QAManager: 用語と定義の明確化

△

2.1, 3, 4, 6.2

パラメータyaml、転記仕様yaml、スキーマyamlなど、多数の外部設定ファイルが処理の根幹をな
しているが、これらのファイルフォーマット（キー、データ型、必須/任意、構造）が定義されていな
い。例えば6.2でプラグインに渡されるJSON文字列のノード一覧はあるが、各ノード（例:
LineageReousceInfo）の内部構造が不明。これでは設定ファイルの作成やレビューが勘に頼る
ことになる。

巻末に付録として「設定ファイル仕様」を追加する。各YAML/JSONファイルについて、全て
のキーを階層構造で示し、それぞれの「キー名」「データ型」「必須/任意」「説明」「設定例」
をテーブル形式で網羅的に記述する。

QA-3

QAManager: 異常系の網羅性

△

4, 6.3.1, 8.1

起動パラメータのチェックやファイルが見つからない場合の考慮はされているが、実行中のI/O
エラー（ディスクフル、ネットワーク障害、権限不足など）に対する振る舞いが定義されていない。
例えば、「4. スキーマyamlファイルのダウンロード」でGCS接続に失敗した場合や、「8.1 DBログ
出力」でDB接続に失敗した場合のリカバリ処理（リトライなど）や終了コードが不明確。

QA-4

QAManager: 処理ロジックの具体性評価
(WDT分析)

×

6.3.2 データコンバート方法

データコンバート方法として`expr`と`expr_js`の2種類が挙げられているが、`expr`について「表
現式の実装は自分で実装する必要です」としか記述がなく、その実装・連携方法が全く不明。ど
のように外部実装を読み込み、実行するのかが定義されていないため、実装もテストも不可能
である。

主要なI/O処理（GCSダウンロード、DB書き込み、Parquetファイル出力）ごとに、異常系の
振る舞いを定義する。
【ゴールデンケース（DB書き込み失敗時）】
1. DBへの接続および書き込み処理で例外が発生した場合、5秒間隔で最大3回リトライす
る。
2. 3回のリトライ後も失敗した場合、処理を中断し、エラーログ（DB接続に失敗した旨、リト
ライ回数、最終的な例外情報）をコンソールおよびローカルファイルに出力する。
3. ツールは専用の終了コード（例: 251）で異常終了する。

`expr`の実現方式を具体的に定義する。
【ゴールデンケース（動的ロード方式）】
`expr`には「アセンブリ名,クラスのフルネーム,メソッド名」を文字列で指定する。ツールはリ
フレクションを用いて指定されたアセンブリをロードし、静的メソッドを呼び出す。そのメソッ
ドのシグネチャ（引数、戻り値の型）を `public static object Convert(object value, DataRow
currentRow)` のように厳密に定義する。アセンブリやメソッドが見つからない場合の例外
処理についても明記する。

対応有無
〇

×

×

×

×

×

対応方法／非対応理由

設計書内に要件定義書・
ルールブックへのハイパーリ
ンクを追加し、追跡性を確保
した。

リソース消費に関する内容は
既に「リソース消費」章で明
記済みであり、追加の章立て
は不要。

　本exeはプラグインを呼び
出す実行体であり、各プラグ
インの処理ロジックは個別仕
様書で管理されるため、本設
計書では対象外。

各設定ファイル
（YAML/JSON）の詳細仕様
は生成ツール側で定義・管
理されており、本設計書では
参照範囲に留める。

本設計書では正常系処理を
中心に記載しており、異常処
理は「例外処理」章に統合し
て記述済み。追加対応は不
要。

exprの実装・連携仕様は「プ
ログラム基本設計書_基礎-
コンポーネント
_ExpressionUtil」で詳細定義
済みのため、本書では概要
のみ記載。





================================================================================
ドキュメント: プログラム基本設計書_累積作成ツール_処理詳細_V8.pdf (PDF 2)
ファイル名: プログラム基本設計書_累積作成ツール_処理詳細_V8.pdf
================================================================================

3-1.処理詳細説明(全体)

プログラムID acumulate-file-loader

システム名 f013カタログ部品データ作成
業務機能名 累積作成
プログラム名称 累積EXE

システムID
バージョン
プロジェクトNo

12000599-00

作成日
更新日

2025/2/14
2025/11/6
区分

作成者
更新者

3H.尚坤
3H.尚坤

汎用

詳細内容

・起動パラメータ
コマンド:

acumulate-file-loader.exe -n workflow-namespace -w workflow-name -g lineage-group-id -s lineage-subgroup-id -t execute-task-id -p project-id -i input-path

パラメータ:

改訂No. 削

1/

短縮形式 長い形式

-n
-w
-g
-s
-t
-p
無し
-i
無し

必須/オプション
必須
--workflow-namespace
必須
--workflow-name
必須
--lineage-group-id
必須
--lineage-subgroup-id
オプション
--execute-task-id
必須
--project-id
オプション　※1・※2
--input-list-file
オプション　※2
--input-file
--input-file-selector             オプション　※2

機能
ネームスペース
ワークフロー名
リネージグループID
リネージサブグループID
タスクID
プロジェクトID
処理ユニットのセット(JSONファイル)
データソースファイルパス
処理対象ファイル選択

備考
Cloudログ出力用のnamespace
ワークフローの名前
データ処理のリネージID
このデータパイプラインの子ワークフローIDです
今回作業のタスクID
GCPで管理・操作するために、一意の識別子（ユニークID）
処理対象の複数のファイル情報が入っている json
データファイルはトリガーとする場合、非バッチ系、1つファイルの情報
プラグインが処理対象のデータ（ファイル）を特定するための選択子です。

※1 バッチ系の場合、処理対象は複数件ある場合、そのリストをJSON形のObject Listにして、

別途方法で、分割処理が必要です。

※2 運用上、この3つオプションのパラメータ（--input-list-file、-i/--input-file、--input-file-selector  ）は排他必要です。

・リソース消費

システムリソース負荷に影響する要因

項目
処理データ量
RowGroupSize
データ分布
特殊変換の複雑さ

説明
処理対象のテーブル数、各テーブルの行数・列数により処理量が増加
バッチ処理一度のデータ量（デフォルト1万）で、RowGroupSize が大きいほどメモリ使用量も増えます。
処理対象のファイルが分散配置されている場合、散在すればするほど検索時間が長くなります
特殊な変換処理が多ければ多いほど、処理時間が長くなり、メモリ消費量も増加します

あるサンプルデータでテスト結果 : 量化指標（実測値に基づくリソース消費の定量化）
データ処理量が増えるごとに、CPU使用率も高くなり、メモリ使用量も増加します。

RowGroupSize

特殊変換

10,000

ない

データ件数
500
5,000
1万
10万
100万
200万

CPU使用率
7%
17%
20%
21%
41%
45%

メモリ使用量(MB) ※1
30 MB
97 MB
134 MB
302 MB
1400 MB
2,505 MB

時間(s)
4 s
46 s
74 s
38 s
334 s
1,004 s

※1本処理で使用されるメモリ量は、Workflow の yaml に定義されたテンプレートのリソース制限（resources.limits.memory など）に依存します。

設定された上限を超えた場合、メモリオーバーフロー（OOM）が発生し、Pod が強制終了される可能性があります。

・処理詳細

1　ツール起動

上記「起動パラメータ」で定義した起動方法で起動して、下記の場合、異常で起動できないです。

＞　必須引数が未設定の場合、

コンソールに以下のエラーメッセージを表示し、ツール起動は失敗します。

Console ： 必須オプション「{optionName}」が未設定です。W/F動作時は定義YAMLテンプレートで引数に指定してください。
Console ： 累積作成ツール(acumulate-file-loader)
Console ： 使い方 : acumulate-file-loader.exe -n ネームスペース -w ワークフロー名 -p GCPプロジェクトID -g リネージグループID -s リネージサブグループID

Console ： Required option '{option}, {optionName}' is missing.

※ CommandLine API出力

＞　必須引数の値が空の場合、

コンソールに以下のエラーメッセージを表示し、ツール起動は失敗します。

Console ： 必須オプション 「{optionName}」 の値が空です。W/F動くの場合、定義Yamlの テンプレートの引数で値が指定されているか確認してください。

＞　三つのオプション引数(-i/--input-file 、 --input-list-file と --input-file-selector)は２つ以上設定された場合、

コンソールに以下のエラーメッセージを表示し、ツール起動は失敗します。

Console ： オプション 「-i/--input-file」, 「--input-list-file」, 「--input-file-selector」は同時に指定できません。どちらか一方のみを指定してください。

＞　上記の必須、オプション以外の引数が設定された場合、

コンソールに以下のエラーメッセージを表示し、ツール起動は失敗します。

Console ： 不明なオプション「{unknown}」が指定されました。入力パラメータを確認してください。

Console ： 累積作成ツール(acumulate-file-loader)

Console ： 使い方 : acumulate-file-loader.exe -n ネームスペース -w ワークフロー名 -p GCPプロジェクトID -g リネージグループID -s リネージサブグループID

Console ： Option '{unknown}' is unknown.

※ CommandLine API出力

2.　ツール初期処理

2.1　バラメータyamlファイルの内容の読込・チェック、リネージ情報の取得
リネージグループ、サブグループ、ワークフローなどを初期化します。

リネージグループID、リネージサブグループIDで、バラメータyamlファイル名を決めて、
・
・

「ワークフローID」　: 「リネージグループID + リネージサブグループID」
「parameter_(ワークフローID).yaml」

ツール実行に必要なリネージ設定情報、各種パラメータを、事前に作成されたパラメータyamlファイルから、リネージの対応情報をリードします。

Copyright(c) 2020 Broadleaf Co.,Ltd.

13
13
13

13
13
13

4
4
4
5
4
4
4
4

4
4
4
4
4
4
4

12

12

6
6

PTE-010-35.0303-01_プログラム基本設計書

・パラメータyamlファイル(parameter_(ワークフローID).yaml)の存在チェック

> パラメータyaml(parameter_(ワークフローID).yaml) が見つかった場合、処理用のリネージ情報を取得します。

ツール実行用のリネージ設定内容をバラメータyamlから取得しておいて

ツール実行に必要なリネージIDを取得します。(バラメータyamlに、ワークフローのリネージサブグループにあるリネージ設定は全て入ってる)
・すべてのLineage情報の取得および設定パラメータの取得

パラメータyamlファイルを読込み、「ParameterInfoEntity」にキャシューします。

> パラメータyaml(parameter_(ワークフローID).yaml) が見つからない場合、

エラーメッセージを表示して、処理中断します。

指定された引数「{lineage-group-id}」と「{lineage-subgroup-id}」で決まったバラメータyamlファイル「{parameterFilePath}」が見つかりません。

※ログ出力の設定すらないため、当該ログはコンソールに出力します。

2.2　ログ初期化

ログ出力の初期化

初期化バラメータ：
project-id
execute_task_id
process_id
workflow-name
name-space
logfilepath
logtypelist
loglevel

GCPリソースを管理・操作する際に使用する、一意の識別子（ユニークID）
実行タスクID
プロセス識別用の一意なID
現在のワークフローの名称
ログ出力用のnamespace
ログ出力ファイルのパス
ログ出力先設定リスト（console, local, cloudlogging）
ログレベル（release, debug）

「workflow_config_common.yml」ファイルに設定できて、LOG出力は下記の3つ設定可能です。

-console
-local
-cloud_logging

コンソールにログを出力します。
ローカルファイルにログを保存します。
CloudLogging 上にログを出力します。

2.3 リネージの処理開始ログ、「データリネージ実行ログ」に開始ログの出力

ツールが処理対象の設定情報(リネージ)を取得するために、リネージID（lineage_id）が必要です。
ツールプロセスID(processId)、引数のタスクID(task_id)を使います。

＞　リネージID（lineage_id）が1件のみ取得できた場合

処理開始ログの出力

処理の開始時に、確定できたリネージIDで、バラメータyamlファイルからリネージ名（process_name）を取得し、下記のInfoタイプログを出力する：

INFORMATION タイプ: 「{process_name}」 を開始します...

「データリネージ実行ログ」への開始ログ出力

本リネージ処理の開始情報を「データリネージ実行ログ」テーブルに出力します。
※環境変数からリソース管理DBの接続情報を取得します。

設定項目

設定内容

№

1

2

3

4

ログインデックスID
リネージグループID
リネージサブグループID
リネージID
適用バージョン

5
6 実行順序
プロセスID
7
8 ワークフローID
実行日
9
10 実行時間
11
12 実行スタータス
詳細メッセージ
13
14 入力ファイル
出力ファイル
15

実行フェーズ

log_index_id
lineage_group_id
lineage_sub_group_id
lineage_id
PGのバージョン番号
lineage_no
PGID
workflow_name
YYYYMMDD
HHMMSSFFFFFF
start:処理開始
info:情報
処理開始提示メッセージ
作業対象ファイルパス
空

＞　上記以外の場合

リネージ情報が特定できない場合、ログを出力し、「8　処理完了ログ出力およびDBログ出力」を実行し、終了する。
＞　リネージID（lineage_id） が1件も取得できなかった場合、エラーログを以下の形式で出力します。

エラーログ

ERROR : パラメータYAML「{parameterYamlFile}」で、プロセスID「{processId}」・タスクID「{taskId}」に該当するリネージ定義は見つかりません。

＞　リネージID（lineage_id） が複数件取得された場合、エラーログを以下の形式で出力します。

エラーログ

ERROR : パラメータYAML「{parameterYamlFile}」で、プロセスID「{processId}」・タスクID「{taskId}」のリネージ定義が重複しています。

上記いずれのケースでも、「データリネージ実行ログ」に異常開始ログを出力します。

※環境変数からリソース管理DBの接続情報を取得します。

№

設定項目

設定内容

3

2

1

ログインデックスID
リネージグループID
リネージサブグループID
リネージID
4
5 適用バージョン
6 実行順序
プロセスID
7
8 ワークフローID
9 実行日
10 実行時間

log_index_id
lineage_group_id
lineage_sub_group_id
空
空
空
PGID
workflow_name
YYYYMMDD
HHMMSSFFFFFF

2/

内容修正、ソース修正必要です。

内容修正、ソース修正必要です。

6
6
6
6
6
6
6
6
6
12

6
6

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
15
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

備考
AUTO_INCREMENT
-
-
-
-
-
-
-
-
-
処理開始ステータス
-
「{process_name}」 を開始します...
-
-

備考
AUTO_INCREMENT
-
-
リネージIDの取得に失敗したため、空とします
リネージID関連項目が取得できなかったため、空とします
リネージID関連項目が取得できなかったため、空とします
-
-
-
-

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

実行フェーズ

11
12 実行スタータス
13 詳細メッセージ
14 入力ファイル
出力ファイル
15

start:処理開始
error：エラー
エラー情報
作業対象ファイルパス
空

上記の処理、データベースに接続する必要があります。

データベース接続できない場合、

エラーログ：

ERRORタイプ：DB関連情報が正しく設定されていません。

2.4 リネージ情報関連のチェック

上記リネージIDが特定できて、リネージ情報の設定をチェックします。

2.4.1 当該リネージIDの入出力情報（リネージの入出力リソース情報）が存在しない場合、エラーログを以下の形式で出力します。

エラーログ

ERROR : 「データリネージ連携テーブル」に当リネージID「{LineageID}」の入出力定義が見つかりません。

2.4.2 拡張機能情報1(extend_function_info_1)が未設定の場合、エラーログを以下の形式で出力します。

エラーログ

ERROR : 「データリネージプロセス管理テーブル」に当リネージID「{LineageID}」の「拡張機能情報1(extend_function_info_1)」が未設定です。

2.4.3 拡張機能情報1が設定されているが、DLLファイルが存在しない場合、エラーログを以下の形式で出力します。

エラーログ

ERROR :  「データリネージプロセス管理テーブル」に当リネージID「{LineageID}」の拡張機能情報1(extend_function_info_1)で指定されたDLLファイル「{ExtendFunctionInfo1}」が見つかりません。

2.4.4 データ圧縮方式（compressMethodExpr）が設定されており、設定書式が不正な場合、エラーログを以下の形式で出力します。

エラーログ

ERROR :  「リソース管理テーブル」に当リネージID「{LineageID}」のデータ圧縮方式（compress_method）の書式が不正です。このようにする必要です：logical:<値>, local:<値>（例：logical:parquet_gzip, local:7z）。

2.4.5 データ論理圧縮方法が設定されており、設定にエラーがある場合、エラーログを以下の形式で出力します。

エラーログ

処理開始ステータス
-
リネージID取得できないの原因、上記リネージID特定できない原因を使う
-
-

14
14
15
14
14

3/

内容修正、ソース修正必要です。

内容修正、ソース修正必要です。

内容修正、ソース修正必要です。

内容修正、ソース修正必要です。

ERROR :  「リソース管理テーブル」に当リネージID「{LineageID}」のデータ論理圧縮方式「{compressMethod}」は、parquet_brotli、parquet_gzip、parquet_lz4、parquet_lzo、parquet_snappy、または parquet_zstd のいずれかである必要があります。

内容修正、ソース修正必要です。

2.4.6 データ物理圧縮方法が設定されており、設定にエラーがある場合、エラーログを以下の形式で出力します。

エラーログ

ERROR :  「リソース管理テーブル」に当リネージID「{LineageID}」のデータ物理圧縮方法「{fileCompressMethod}」は、7z または zip のいずれかである必要があります。

内容修正、ソース修正必要です。

3 リネージIDに基づいて転記仕様yamlファイルの確定と読み込み

 指定されたリネージIDに基づき、対応する転記仕様YAMLファイル（transform_spec.(リネージID).yaml）を特定し、パースします。
※　ワークフローで使用する転記仕様 yaml ファイルは、Argo Workflows の artifacts 機能により、事前に共有・参照可能な状態となっています。

プロセスログ

PROCESS タイプ： 転記仕様YAMLファイル「{transSpecYamlPath}」の読み込み 実行

転記仕様yamlファイル（transform_spec_(リネージID).yaml）の内容を読み取っておいて

・転記仕様yamlファイルが見つかりました、

転記仕様yaml「transform_spec_(リネージID).yaml」を見て、このリネージで指定した転記情報を取得します。

・ 転記仕様yamlファイルが見つからないと、処理中断します。

エラーログ

ERROR タイプ: 転記仕様YAMLファイル「{transSpecYamlPath}」が見つかりません。

4　入力、出力データファイルのレイアウト定義用スキーマyamlファイルのダウンロード

Input、Outputスキーマyamlファイルをダウンロードします。

パラメータyaml、転記仕様yamlから操作対象ファイルの定義情報を抽出し、指定GCSから、当該スキーマyamlファイルをダウンロードします。

プロセスログ

PROCESS タイプ： 入力・出力ファイルのスキーマ YAML ファイルのダウンロード 実行

まず、各スキーマyamlファイルの保存箇所を取得して、

バラメータyamlファイル(parameter_(ワークフローID).yaml)から、下記の情報を取得します：

・bucket: スキーマyamlファイルが保存されているGCSのバケット名。
ノード「setting_info」のサブノード「GCS」のサブノード「bucket」

・schemayamlpath: スキーマyamlファイルのパス。

ノード「setting_info」のサブノード「GCS」のサブノード「schemayamlpath」

 次に、対象のスキーマyamlファイル名を取得して

転記仕様yamlファイル(transform_spec_(リネージID).yaml)にから、下記の情報を取得します：

・Inputスキーマyamlファイル

ノード「input_file」の全てのサブノード「schemafilepath」

・Outputスキーマyamlファイル

ノード「output_file」の全てのサブノード「schemafilepath」

取得した bucket、schemayamlpath と Inputスキーマyamlファイル名、Outputスキーマyamlファイル名 を利用して、GCSから対象のスキーマyamlファイルをダウンロードします。
ダウンロードに失敗した場合、共通DLLがダウンロード失敗のログメッセージを出力します。

5　処理対象リスト作成

起動パラメータ（-i/--input-file 、 --input-list-file 、--input-file-selector ）で指定された処理対象に対して、

Copyright(c) 2020 Broadleaf Co.,Ltd.

DownloadSchemaFile
パラメータ：

・転記仕様yaml
・ワークフロー設定情報

機能：

lineage で使用される可能性のある schema yaml ファイルのダウンロード

内容修正、ソース修正必要です。

内容修正、ソース修正必要です。

7
7
14
14

14
14

5
5

5
5

PTE-010-35.0303-01_プログラム基本設計書

本リネージの設定に基づきグループ化を行い、処理を実行します。

> パラメータ (-i/--input-file) だけを設定した場合

入力ファイル名が命名規則に従っているかのチェック

＞ 入力ファイル名が命名規則に従っていない場合：

エラーログを出力する（ExitCode：244）

ERROR タイプ： 対象データファイル「{excelFilePath}」は、当リネージの命名規則「{fileNameRule}」に適合しません。

以下の処理を実行する：

・ 単一ファイルを対象リストに格納します。

> パラメータ (input-file-selector ) だけを設定した場合

以下の処理を実行する：

・ 処理対象を対象リストに格納します。

> パラメータ (--input-list-file) だけを設定した場合

以下の処理を実行する：

· JSON ファイルを読み込み、対象リストに格納します。

> -i/--input-file、--input-list-file、および --input-file-selector のいずれも設定されていない場合

以下の処理を実行する：

・ 「repository 」と「filenamerule」に基づき、対象ファイルリストに格納します。

プロセスログ

PROCESS タイプ： 処理対象リスト：{objList}

6　対象データファイルから、データをリードして、データコンバートを実行し、Parquetファイルを作成します。

データリード、データコンバート、Parquetファイルに書きこみの流れで実施します。

6.1 Inputスキーマ、Outスキーマyamlファイルのリード

コンバート処理開始時に、InputスキーマyamlファイルおよびOutputスキーマyamlファイルを解析し、必要な情報を取得します。

① Inputスキーマyamlファイルの解析

プロセスログ

PROCESS タイプ： 入力スキーマ YAML「{SchemaFilePath}」ファイルを解析し、標準転記の表現式　取得

・Before表現式の取得

special_transformノードに値が設定されている場合、そのカラムのBeforeに対応する辞書に表現式を格納します。

② Outputスキーマyamlファイルの解析

プロセスログ

PROCESS タイプ： 出力スキーマ YAML「{SchemaFilePath}」ファイルを解析し、標準転記の表現式　取得

・Main/After表現式の取得

special_transformノードに値が設定されている場合、そのカラムのMain/Afterに対応する辞書に表現式を格納します。

・RowGroupKeyの取得

RowGroupKeyノードに値が設定されている場合、その値を取得し記録します。

・RowGroupSizeの取得

RowGroupSizeノードに値が設定されている場合、その値を取得し記録します。
RowGroupSizeノードに値が設定されていない場合、デフォルトのバッチ処理は次の条件で実行されます：

①、データが500MBのメモリを占有した場合（優先判定）
②、上記に該当せず、行数が1万行に達した場合

6.2　コールバック駆動のデータ取込・正規化（to Inputスキーマ）

データを行単位で読み込み、検証・正規化後、Inputスキーマ準拠レコードをコールバックで逐次返却する方式を定義します。

 「データリネージプロセス管理テーブル」に基づき、リネージID「{LineageID}」の「拡張機能情報1（extend_function_info_1）」で指定されたプラグインを使用して、
データファイルを処理します。

プラグインの指定取得:

「parameter_（ワークフローID）.yaml」ファイルに、リネージ「リネージID」ノードの「ExtendFunctionInfo1」の設定値で判断します。

・プラグインが指定された場合、プラグインの対象データファイルを読み込みます。

下記のデータタイプに対しては、それぞれのデータ形式やレイアウトに応じたプラグインを設計し、
デリゲートコールバック方式でデータレコードを返すようにします。

・Excel
・PDF
・HTML
・Parquet
・TSV/CSV/固定長

←専用
←専用
←専用
←汎用、B145に実装します
←汎用、B145にテキストファイル解析プラグインとして実装します

※上記以外のデータタイプや解析形式を将来的に追加する場合は、プラグインを新規作成するだけで対応可能とします。

振分モジュール経由で、指定されたプラグインを起動します。

引数:

※あくまで、現状の構造で推測した数値です。
　テスト&運用上で修正する必要かもしれません。

・onDataRow デリゲート
・logUtil LogUtil インスタンス  プラグインにもログ出力するために、追加して
・JSON形式の文字列

処理済みのDataRowを受け取るデリゲート

リネージ関連の各種情報

4/

内容修正、ソース修正必要です。

内容修正、ソース修正必要です。

LineageId明确指定了Plugin  DLL 名称：

4
4
3

3

6
6

6

14

6
6
6
6

14

6
6
6
6
6
6
6
6
8
8
8

12
12

5

6
12

14
14
14
14
14
14

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

詳細については、「プログラム基本設計書_プラグイン_振分モジュール_解析加工IF定義」を参照してください。

※JSON文字列には、以下の構造でデータ処理パイプラインの設定情報が含まれています。
ノード名
LineageGroupID
LineageSubGroupId
LineageId
LineageSummary
LineageReousceInfo
TransformyamlInfoDic
DataFileList
MacroVariable
WorkFlowSettings
EventFileContent

説明
リネージグループID
リネージサブグループID
リネージID
リネージ関連情報
リネージリソース情報
入力ファイル情報辞書
入力ファイルリスト
マクロ変数
ワークフロー設定情報
イベントファイル内容

詳細内容
データ処理のリネージグループID
このデータパイプラインの子ワークフローIDです
呼び出し元のリネージ定義 LineageId
リネージのプラグイン、プロセス名称などの設定情報
入出力リソースのエンコーディング、入出力区分、圧縮方式などの設定情報
入力ファイルのrepository、ファイル名称ルール、スキーマyaml名等設定情報
対象ファイルフルパスリスト
マクロ変数定義(マクロ変数名と正則表現)
ログ出力、GCSパス、作業・出力ディレクトリなど、ワークフロー実行に必要な設定
処理実行の契機となるイベント設定。ジョブ情報、通知先メールアドレス、通知API、などを含む。

・プラグインの設定がない場合、

設定不正として、エラーとして、累積ツールを異常終了（255）にします。
エラーログ：

ERROR  タイプ : 「データリネージプロセス管理」で、当リネージID「{0}」の「拡張機能情報1(extend_function_info_1)」が未設定です。

6.3　上記ロードできたデータレコードに対し、データコンバートをしてから、Parquetファイルに出力

メモリコストを控えるために、RowGroupSizeでメモリキャシューをしてから、データコンバート～Parquetファイル出力を行います。

6.3.1　Parquetファイル生成方式の分岐

OutputスキーマyamlファイルのRowGroupKey設定の有無によるParquetファイル生成方式の条件分け

・RowGroupKeyが設定されていない場合、直接Parquetファイルを生成
・RowGroupKeyが設定されている場合、DuckDBを経由してParquetファイルを生成

＞　RowGroupKeyを設定していない場合、直接Parquetファイルに出力

OutputスキーマyamlファイルのRowGroupSizeで、Parquetファイルに出力

1) データコンバート(入力レコード⇒出力レコードの変換)

Inputスキーマカラム操作、転記仕様yamlのマッピング実施、Outputスキーマカラム操作をして

「6.3.2　データコンバート方法」を参照してください。

エラーが発生した場合、以下の形式でメッセージを出力します。

ERROR/WARNING  タイプ :エラーファイル: {filepath}
ERROR/WARNING  タイプ :エラー行番号: {num}
ERROR/WARNING  タイプ :エラー明細: 列「{colname}」内容「{data}」は定義最大長さ「{Length}」を超えています。
ERROR/WARNING  タイプ :エラー明細: データ「{data}」を指定「{type}」に変換できません。
ERROR/WARNING  タイプ :エラー明細: データ列 「{colName}」は NULL を許可していないため、NULL 値は設定できません。

2) Parquetファイル出力

上記データリスト(RowGroupSize)で、コンバートできたデータをParquetファイルに追加モードで出力します。

「6.3.3　Parquetファイル出力共通設定」を参照してください。

プロセスログ

PROCESS タイプ： 「{rowCount}」件のレコード コンバートできて、出力ファイルに「{rowCount}」件のレコード 追加

＞　RowGroupKeyを設定している場合、DuckDBにキャシューして、Parquetファイルに出力

DuckDBへのデータ追加のバッチ処理は、次の条件で実行されます：

①、 データが500MBのメモリを達した場合（優先判定）
②、 上記に該当せず、行数が10万行に達した場合

・データコンバート

1) データコンバート(入力レコード⇒出力レコードの変換)

Inputスキーマカラム操作、転記仕様yamlのマッピング実施、Outputスキーマ操作をして

「6.3.2　データコンバート方法」を参照してください。

データ変換・データチェック

① NULL 非許可の項目に NULL 値が設定された場合に出力します。

ERROR/WARNING タイプ : エラーファイル : {filepath}
ERROR/WARNING タイプ : エラー行番号 : {num}
ERROR/WARNING タイプ : エラー明細 : データ列「{colName}」は NULL を許可していないため、NULL 値を設定できません。

② 入力データを定義されたデータ型に変換できなかった場合に出力します。

ERROR/WARNING タイプ : エラーファイル : {filepath}
ERROR/WARNING タイプ : エラー行番号 : {num}
ERROR/WARNING タイプ : エラー明細 : データ「{data}」を指定された型「{type}」に変換できません。

③ 入力データの項目値が、定義された最大長を超過した場合に出力します。

ERROR/WARNING タイプ : エラーファイル : {filepath}
ERROR/WARNING タイプ : エラー行番号 : {num}
ERROR/WARNING タイプ : エラー明細 : 列「{colname}」の内容「{data}」は定義された最大長「{Length}」を超えています。

2) コンバートできたデータを DuckDB にインポート（ディスクベース）

対象の コンバートできたデータを DuckDB にインポートし、中間テーブルとして保存します。
メモリ上ではなく、ディスクベースで処理を行うことで、大容量データにも対応可能とします。

Copyright(c) 2020 Broadleaf Co.,Ltd.

※あくまで、現状の構造で推測した数値です。
　テスト&運用上で修正する必要かもしれません。

14

6
6
6
6
6
6
6
6
6
6
6
6

5
9
9
12
14

5

7

7
7

7

7

7

7
7

14

7
8
8
8
7

7

7

7

5/

内容修正、ソース修正必要です。

PTE-010-35.0303-01_プログラム基本設計書

プロセスログ

PROCESS タイプ： 「{rowCount}」件のレコード コンバートできて、DuckDBに「{rowCount}」件のレコード 追加

· Parquetファイル出力

指定RowGroupKeyで、Parquetファイルを生成

Outputスキーマyamlファイルに「RowGroupKey」の指定がある場合だけ、指定キーでもう一回グループして、Parquetファイルの生成を行います。

※データコンバートできたら、更にRowGroupKeyでグループする必要です。

1)　RowGroupKey によるグループ化処理

DuckDB 上のテーブルに対し、RowGroupKeyでデータをグループ化します。
グループ単位で再構成されたデータセットを、SQL を用いて取得・加工します。
※メモリコストを考慮した上で、実装する必要です。

2)　Parquetファイルの生成

＞　RowGroupSizeを設定している場合

RowGroupKey により分割された各グループ内で、 RowGroupSize 件数単位に分割。
分割されたデータごとに、Parquet ファイルに追加モードで出力します。

＞　RowGroupSizeを設定していない場合

RowGroupKey により分割されたデータごとに、Parquet ファイルに追加モードで出力します。

「6.3.3　Parquetファイル出力共通設定」を参照してください。

6.3.2　データコンバート方法

プラグインからonDataRow経由で受け取った各レコードを、指定された転記仕様YAMLに基づき、
リアルタイムでデータコンバートを実施します。

1)Inputスキーマyamlに、カラム表現式にBefore指定あり項目に対して、更にコンバートします。

元データに、ブリ処理をする必要か否かは、カラムの表現式にbefore計算式で記載してるので、

Inputスキーマyamlにカラム表現式に、「Before」変換式があるカラムに対し、指定表現式でコンバートを行う。

＞ Before表現式が設定ありカラム

カラム毎に、Beforeカラム表現式の操作をします。

※表現式の書き方は、下記2種類があります。 詳細は、「プログラム基本設計書_基礎-コンポーネント_ExpressionUtil」に参照

・expr

表現式の実装は自分で実装する必要です。

・expr_js

表現式の実行は、自分で実装する必要なし、規格に則った記述にすれば、JavaScript エンジンがその表現式を解釈し、実行してくれます。
Jint（ジント）は、.NET アプリケーション上で JavaScript を実行するための ECMAScript エンジン です。
C# など .NET 言語から JavaScript を呼び出して実行できる、軽量で高性能な JavaScript インタプリタです。

2) プリ処理できたレイアウトを出力レイアウトへのマッピングとデータ転送

コンバート処理が完了したデータは、出力レイアウトにマッピングしたら、データを転送する

Outputスキーマyamlファイルに設定された「ITEM_MATCH_MODE」に基づいて、入力解析結果のデータと出力レイアウトの間でカラムマッピング
※詳細は、ルールブックの「2. 入力ファイルと出力ファイルの列マッピングルール（ITEM_MATCH_MODE）」を参照。

3) Outputスキーマyamlに、カラム表現式にMain/After指定あり項目に対して、更にコンバートします。

Main/After変換辞書に記録されている各カラムおよび対応する表現式を使って、「Main→After」の順に変換を行う。

＞　Main/After表現式が設定されているカラム

・カラム毎に、Mainカラム表現式の操作をします。
・カラム毎に、Afterカラム表現式の操作をします。

6.3.3　Parquetファイル出力共通設定

Parquetファイルについて、出力方法と圧縮モードもバラメータyamlから取得して、

・出力処理方式（io_div）

GCSストリームにするか、ローカルに出力するかを判断します。
この io_divは、パラメータyamlファイルに、「lineage_info」ノード配下当該リネージIDノード配下の 「out」ノード → 「io_div」ノードに設定しています。

※詳細は、ルールブックのシート「3)DBの基本設定の説明」内の「3-1　入出力区分（io_div）」を参照。

・論理圧縮（compress_method）

Parquetファイルの出力する際の論理圧縮方法を決定します。
論理圧縮の方法は、パラメータyamlファイルに、「lineage_info」ノード配下当該リネージIDノード配下の 「out」ノード→ 「compress_method」ノードに指定してます。

※詳細は、ルールブックのシート「3)DBの基本設定の説明」内の「5-5　圧縮方式(compress_method)」を参照。

7　作成できたParquetファイルの物理圧縮

プロセスログ

PROCESS タイプ： 出力ファイルの物理圧縮 実行

物理圧縮指定有り場合、共通の圧縮方法を使って、Parquetファイルの圧縮を実施します。

共通「DwhFileProcessUtil」を利用して、出力ファイルを圧縮します。
file:物理圧縮方法

圧縮なしの場合は、noneを設定しますか、空を設定します

設定内容

Parquet作成後の物理圧縮方法

Copyright(c) 2020 Broadleaf Co.,Ltd.

14

7
7
7
7

7
7
7
7
7
7
7
7
7
7
7
7
7

14
14

7
6
6
6
6
6
6
6
6
6
6
6
6
6

7
5
5
5
5
5
5
7
6
6
6
6
6

7
7
7
7
7
7
7
7
7
7
7
7
7
7
7

14
14

6/

内容修正、ソース修正必要です。

所有的Batch都完

PTE-010-35.0303-01_プログラム基本設計書

空白 または file:none
file:zip
file:7z
...

圧縮しない
zip
7z
...

※物理圧縮の場合、パスワード指定有りの場合、暗号化かけて、圧縮します。

INFORMATIONログ

INFORMATION タイプ：出力ファイル：{outputFilePath}
INFORMATION タイプ：出力レコード件数: {recordCount}

8　処理完了ログ出力およびDBログ出力

8.1　「データリネージ実行ログ」テーブルに、今回実行の結果をログに出力する。
処理完了の基本情報を「データリネージ実行ログ」テーブルに出力します。
※環境変数からリソース管理DBの接続情報を取得

ログインデックスID
リネージグループID
リネージサブグループID
リネージID
適用バージョン

№ 設定項目
1
2
3
4
5
6 実行順序
プロセスID
7
8 ワークフローID
実行日
9
10 実行時間
11

実行フェーズ

設定内容
log_index_id
lineage_group_id
lineage_sub_group_id
lineage_id
PGのバージョン番号
lineage_no
PGID
workflow_name
YYYYMMDD
HHMMSSFFFFFF
end:処理終了

12 実行スタータス

ok:正常終了,warning:警告,error:エラー

13

詳細メッセージ

処理結果情報、エラー情報等

14 入力ファイル
15
出力ファイル

作業対象ファイルパス
Parquetファイルパス

8.2　ログ出力

処理終了ログの出力

処理の終了時に、リネージ名（process_name）を使用して、以下の形式でログを出力する：

> 正常完了

INFORMATION タイプ: 「{process_name}」が正常完了しました。

> 異常終了

WARNING タイプ: 「{process_name}」が異常終了しました。(警告：{exitcode})
ERROR タイプ: 「{process_name}」が異常終了しました。(エラー：{exitcode})
ERROR タイプ： {ex.StackTrace}

転記仕様yamlの「loginfo」の設定により、転記仕様yamlの「logtype」で指定された方法（コンソールログ、ローカルファイル、クロードロギング）を利用してログを出力します

ログタイトル：

2024/02/01 16:53:11 [情報] (NameSpace: ネームスペース WorkFlowName: ワークフロー名 ExecuteTaskId:実行タスクID)

ログ内容：

・基本な設定情報

リネージグループID、リネージID、PGID、execute_task_idなど

・ファイル情報

入出力ファイルパス、途中作成ファイル（パラメータyaml、転記仕様yaml）など情報

・設定情報のチェック結果
設定不正な情報

・開始、終了情報

各処理ステープの開始、終了情報、exit codeなど

logの例：

2024/02/01 16:53:11 [情報  ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 実行時単位ID(リネージグループID-リネージID) :L00005
2024/02/01 16:53:11 [情報  ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) ツールID :acumulate-file-loader
2024/02/01 16:53:11 [情報  ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) yaml設定内容のチェックが開始しました...
2024/02/01 16:53:11 [情報  ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 転記仕様スキーマ：D:\tmp\transform_spec.yml
2024/02/01 16:53:11 [情報  ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) ツール実行前チェックを行います...
2024/02/01 16:53:11 [情報  ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)
2024/02/01 16:53:11 [情報  ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) yaml設定内容のチェックが正常完了しました。
2024/02/01 16:53:11 [エラー ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 転記仕様にoutfileのfilenameruleの設定が不正ため、下記のinfileに対しますoutfileの名称が重複になります。
2024/02/01 16:53:11 [エラー ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)  ・t001_f0002_3321B0_11-03_001_parts_ver0001.html ⇒ 3321B0_11-03_parts_202311_ver001.Parquet
2024/02/01 16:53:11 [エラー ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)  ・t001_f0002_3321B0_11-03_001_parts_ver0002.html ⇒ 3321B0_11-03_parts_202311_ver001.Parquet
2024/02/01 16:53:11 [警告  ] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 累積処理が異常終了しました。(警告：99)

Copyright(c) 2020 Broadleaf Co.,Ltd.

7/

14
14
14

11

備考
AUTO_INCREMENT
-
-
-
-
-
-
-
-
-
-

複数種類のエラーが検知された場合、
エラーハンドリング設定に従い、ExitCodeが最後のエラー情報を出力する。

※最後のエラー情報とは、以下の2種類のうち、最後に出力された Msg を指します。
  ① 振り分けモジュールで呼び出された Plugin の After 処理の戻り値（Msg）
  ② exe 実行中に発生したエラー情報

※512文字まで残って、それ以上、切り捨て

-
-

PTE-010-35.0303-01_プログラム基本設計書

8/

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書





================================================================================
ドキュメント: プログラム基本設計書_累積作成ツール_処理詳細_V7.pdf (今回の設計書 V7)
ファイル名: プログラム基本設計書_累積作成ツール_処理詳細_V7.pdf
================================================================================

3-1.処理詳細説明(全体)

プログラムID acumulate-file-loader

システム名 f013カタログ部品データ作成
業務機能名 累積作成
プログラム名称 累積EXE

システムID
バージョン
プロジェクトNo

12000599-00

作成日
更新日

2025/2/14
2025/10/30
区分

作成者
更新者

3H.尚坤
3H.尚坤

汎用

詳細内容

・起動パラメータ
コマンド:

acumulate-file-loader.exe -n workflow-namespace -w workflow-name -g lineage-group-id -s lineage-subgroup-id -t execute-task-id -p project-id -i input-path

1/

改訂No. 削

備考
Cloudログ出力用のnamespace
ワークフローの名前
データ処理のリネージID
このデータパイプラインの子ワークフローIDです
今回作業のタスクID
GCPで管理・操作するために、一意の識別子（ユニークID）
処理対象の複数のファイル情報が入っている json
データファイルはトリガーとする場合、非バッチ系、1つファイルの情報
プラグインが処理対象のデータ（ファイル）を特定するための選択子です。

パラメータ:

短縮形式 長い形式

-n
-w
-g
-s
-t
-p
無し
-i
無し

--workflow-namespace
--workflow-name
--lineage-group-id
--lineage-subgroup-id
--execute-task-id
--project-id
--input-list-file
--input-file
--input-file-selector

必須/オプション
必須
必須
必須
必須
オプション
必須
オプション　※1・※2
オプション　※2
オプション　※2

機能
ネームスペース
ワークフロー名
リネージグループID
リネージサブグループID
タスクID
プロジェクトID
処理ユニットのセット(JSONファイル)
データソースファイルパス
処理対象ファイル選択

※1 バッチ系の場合、処理対象は複数件ある場合、そのリストをJSON形のObject Listにして、

別途方法で、分割処理が必要です。

※2 運用上、この3つオプションのパラメータ（--input-list-file、-i/--input-file、--input-file-selector  ）は排他必要です。

・リソース消費

システムリソース負荷に影響する要因

項目
処理データ量
RowGroupSize
データ分布
特殊変換の複雑さ

説明
処理対象のテーブル数、各テーブルの行数・列数により処理量が増加
バッチ処理一度のデータ量（デフォルト1万）で、RowGroupSize が大きいほどメモリ使用量も増えます。
処理対象のファイルが分散配置されている場合、散在すればするほど検索時間が長くなります
特殊な変換処理が多ければ多いほど、処理時間が長くなり、メモリ消費量も増加します

あるサンプルデータでテスト結果 : 量化指標（実測値に基づくリソース消費の定量化）
データ処理量が増えるごとに、CPU使用率も高くなり、メモリ使用量も増加します。

RowGroupSize

特殊変換

10,000

ない

データ件数
500
5,000
1万
10万
100万
200万

CPU使用率
7%
17%
20%
21%
41%
45%

メモリ使用量(MB) ※1
30 MB
97 MB
134 MB
302 MB
1400 MB
2,505 MB

時間(s)
4 s
46 s
74 s
38 s
334 s
1,004 s

※1本処理で使用されるメモリ量は、Workflow の yaml に定義されたテンプレートのリソース制限（resources.limits.memory など）に依存します。

設定された上限を超えた場合、メモリオーバーフロー（OOM）が発生し、Pod が強制終了される可能性があります。

・処理詳細

1　ツール起動

上記「起動パラメータ」仕様を参照して起動します。

＞　必要の引数が未設定の場合、

コンソールエラーメッセージ「必須オプション「xxxx」が未設定です。W/F動作時は定義YAMLテンプレートで引数に指定してください。」を表示して、ツール起動に失敗

> 排他の引数(-i/--input-file と --input-list-file と --input-file-selector)が同時設定の場合、

コンソールエラーメッセージ「オプション 「-i/--input-file」, 「--input-list-file」, 「--input-file-selector」は同時に指定できません。どちらか一方のみを指定してください。」を表示して、ツール起動に失敗

13
13
13

13
13
13

4
4
4
5
4
4
4
4

4
4
4
4
4
4
4

12

12

2.　ツール初期処理

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

2.1　バラメータyaml設定内容の読込・チェック、リネージ情報の取得

リネージグループ、サブグループ、ワークフローなどを初期化します。

リネージグループID、リネージサブグループIDで、バラメータyamlファイル名を決めて、
・
・

「ワークフローID」　: 「リネージグループID + リネージサブグループID」
「parameter_(ワークフローID).yaml」

ツール実行に必要なリネージ設定情報、各種パラメータを、事前に作成されたパラメータyamlファイルから、リネージの対応情報をリードします。

・パラメータyamlファイル(parameter_(ワークフローID).yaml)の存在チェック

> パラメータyaml(parameter_(ワークフローID).yaml) が見つかった場合、処理用のリネージ情報を取得します。

ツール実行用のリネージ設定内容をバラメータyamlから取得しておいて

ツール実行に必要なリネージIDを取得します。(バラメータyamlに、ワークフローのリネージサブグループにあるリネージ設定は全て入ってる)
・すべてのLineage情報の取得および設定パラメータの取得

パラメータyamlファイルを読込み、「ParameterInfoEntity」にキャシューします。

> パラメータyaml(parameter_(ワークフローID).yaml) が見つからない場合、

エラーメッセージ「指定されたパラメータファイル「xxxx」が見つかりません。」を表示して、処理中断します。
※ログ出力の設定すらもないなので、コンソール画面にこのログを出力します。

2.2　ログ初期化

ログ出力の初期化

初期化バラメータ：
project-id
execute_task_id
process_id
workflow-name
name-space
logfilepath
logtypelist
loglevel

GCPリソースを管理・操作する際に使用する、一意の識別子（ユニークID）
実行タスクID
プロセス識別用の一意なID
現在のワークフローの名称
ログ出力用のnamespace
ログ出力ファイルのパス
ログ出力先設定リスト（console, local, cloudlogging）
ログレベル（release, debug）

「workflow_config_common.yml」ファイルに設定できて、LOG出力は下記の3つ設定可能です。

-console
-local
-cloud_logging

コンソールにログを出力します。
ローカルファイルにログを保存します。
CloudLogging 上にログを出力します。

2.3 リネージの処理開始ログ、「データリネージ実行ログ」に開始ログの出力

ツールが処理対象の設定情報(リネージ)を取得するために、リネージID（lineage_id）が必要です。
ツールプロセスID(processId)、引数のタスクID(task_id)を使います。

＞　リネージID（lineage_id）が1件のみ取得できた場合

処理開始ログの出力

処理の開始時に、リネージ名（process_name）を使用して、以下の形式でログを出力する：

INFO : 「{process_name}」 を開始します...

「データリネージ実行ログ」への開始ログ出力

本リネージ処理の開始情報を「データリネージ実行ログ」テーブルに出力します。

※環境変数からリソース管理DBの接続情報を取得します。

設定項目

設定内容

№

1

2

3

4

ログインデックスID
リネージグループID
リネージサブグループID
リネージID

Copyright(c) 2020 Broadleaf Co.,Ltd.

log_index_id
lineage_group_id
lineage_sub_group_id
lineage_id

備考
AUTO_INCREMENT
-
-
-

2/

6
6
6
6
6
6
6
6
6
6
6
12
6
6

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

PTE-010-35.0303-01_プログラム基本設計書

適用バージョン

5
6 実行順序
プロセスID
7
8 ワークフローID
実行日
9
10 実行時間
11
12 実行スタータス
詳細メッセージ
13
14 入力ファイル
出力ファイル
15

実行フェーズ

PGのバージョン番号
lineage_no
PGID
workflow_name
YYYYMMDD
HHMMSSFFFFFF
start:処理開始
info:情報
処理開始提示メッセージ
作業対象ファイルパス
空

-
-
-
-
-
-
処理開始ステータス
-
-
-
-

＞　上記以外の場合

＞　リネージID（lineage_id） が1件も取得できなかった場合、エラーログを以下の形式で出力します。

エラーログ

ERROR : パラメータYAML「{parameterYamlFile}」で、プロセスID「{processId}」・タスクID「{taskId}」に該当するリネージ定義は見つかりません。

＞　リネージID（lineage_id） が複数件取得された場合、エラーログを以下の形式で出力します。

エラーログ

ERROR : パラメータYAML「{parameterYamlFile}」で、プロセスID「{processId}」とタスクID「{taskId}」のリネージ定義が重複しています。

上記いずれのケースでも、「データリネージ実行ログ」に異常開始ログを出力します。

※環境変数からリソース管理DBの接続情報を取得します。

№

設定項目

設定内容

3

1

2

ログインデックスID
リネージグループID
リネージサブグループID
リネージID
4
5 適用バージョン
6 実行順序
プロセスID
7
8 ワークフローID
9 実行日
10 実行時間
11
12 実行スタータス
13 詳細メッセージ
14 入力ファイル
出力ファイル
15

実行フェーズ

log_index_id
lineage_group_id
lineage_sub_group_id
空
空
空
PGID
workflow_name
YYYYMMDD
HHMMSSFFFFFF
start:処理開始
error：エラー
空
作業対象ファイルパス
空

備考
AUTO_INCREMENT
-
-
リネージIDの取得に失敗したため、空とします
リネージID関連項目が取得できなかったため、空とします
リネージID関連項目が取得できなかったため、空とします
-
-
-
-
処理開始ステータス
-
-
-
-

3/

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

3 リネージIDに基づいて転記仕様yamlファイルの確定と読み込み

 指定されたリネージIDに基づき、対応する転記仕様YAMLファイル（transform_spec.(リネージID).yaml）を特定し、パースします。
※　ワークフローで使用する転記仕様 yaml ファイルは、Argo Workflows の artifacts 機能により、事前に共有・参照可能な状態となっています。

プロセスログ

PROCESS タイプ： 転記仕様のYAMLファイル「{transSpecYamlPath}」の読み込み 実行

転記仕様yamlファイル（transform_spec_(リネージID).yaml）の内容を読み取っておいて

・転記仕様yamlファイルが見つかりました、

転記仕様yaml「transform_spec_(リネージID).yaml」を見て、このリネージで指定した転記情報を取得します。

・ 転記仕様yamlファイルが見つからないと、処理中断します。

エラーログ

Copyright(c) 2020 Broadleaf Co.,Ltd.

ERROR タイプ: 転記仕様ファイル「{transSpecYamlPath}」が見つかりません。

7
7
14
14

PTE-010-35.0303-01_プログラム基本設計書

4　入力、出力データファイルのレイアウト定義用スキーマyamlファイルのダウンロード

Input、Outputスキーマyamlファイルをダウンロードします。

パラメータyaml、転記仕様yamlから操作対象ファイルの定義情報を抽出し、指定GCSから、当該スキーマyamlファイルをダウンロードします。

プロセスログ

PROCESS タイプ： 入力・出力ファイルのスキーマ YAML ファイルのダウンロード 実行

まず、各スキーマyamlファイルの保存箇所を取得して、

バラメータyamlファイル(parameter_(ワークフローID).yaml)から、下記の情報を取得します：

・bucket: スキーマyamlファイルが保存されているGCSのバケット名。
ノード「setting_info」のサブノード「GCS」のサブノード「bucket」

・schemayamlpath: スキーマyamlファイルのパス。

ノード「setting_info」のサブノード「GCS」のサブノード「schemayamlpath」

 次に、対象のスキーマyamlファイル名を取得して

転記仕様yamlファイル(transform_spec_(リネージID).yaml)にから、下記の情報を取得します：

・Inputスキーマyamlファイル

ノード「input_file」の全てのサブノード「schemafilepath」

・Outputスキーマyamlファイル

ノード「output_file」の全てのサブノード「schemafilepath」

DownloadSchemaFile
パラメータ：

・転記仕様yaml
・ワークフロー設定情報

機能：

lineage で使用される可能性のある schema yaml ファイルのダウンロード

取得した bucket、schemayamlpath と Inputスキーマyamlファイル名、Outputスキーマyamlファイル名 を利用して、GCSから対象のスキーマyamlファイルをダウンロードします。

5　処理対象リスト作成

起動パラメータ（-i/--input-file 、 --input-list-file 、--input-file-selector ）で指定された処理対象に対して、
本リネージの設定に基づきグループ化を行い、処理を実行します。

> パラメータ (-i/--input-file) だけを設定した場合

入力ファイル名が命名規則に従っているかのチェック

＞ 入力ファイル名が命名規則に従っていない場合：

エラーログを出力する（ExitCode：244）

ERROR タイプ： 対象データファイル「{excelFilePath}」は、当リネージの命名規則「{fileNameRule}」に適合しません。

以下の処理を実行する：

・ 単一ファイルを対象リストに格納します。

> パラメータ (input-file-selector ) だけを設定した場合

以下の処理を実行する：

・ 処理対象を対象リストに格納します。

> パラメータ (--input-list-file) だけを設定した場合

以下の処理を実行する：

· JSON ファイルを読み込み、対象リストに格納します。

> -i/--input-file、--input-list-file、および --input-file-selector のいずれも設定されていない場合

以下の処理を実行する：

・ 「repository 」と「filenamerule」に基づき、対象ファイルリストに格納します。

6　対象データファイルから、データをリードして、データコンバートを実行し、Parquetファイルを作成します。

データリード、データコンバート、Parquetファイルに書きこみの流れで実施します。

Copyright(c) 2020 Broadleaf Co.,Ltd.

6.1 Inputスキーマ、Outスキーマyamlファイルのリード

4/

14
14

5
5

5
5

4
4
3
3

PTE-010-35.0303-01_プログラム基本設計書

6

コンバート処理開始時に、InputスキーマyamlファイルおよびOutputスキーマyamlファイルを解析し、必要な情報を取得します。

① Inputスキーマyamlファイルの解析

プロセスログ

PROCESS タイプ： 入力スキーマ YAML「{SchemaFilePath}」ファイルを解析し、標準転記の表現式を取得します...

・Before表現式の取得

special_transformノードに値が設定されている場合、そのカラムのBeforeに対応する辞書に表現式を格納します。

② Outputスキーマyamlファイルの解析

プロセスログ

PROCESS タイプ： 出力スキーマ YAML「{SchemaFilePath}」ファイルを解析し、標準転記の表現式を取得します...

・Main/After表現式の取得

special_transformノードに値が設定されている場合、そのカラムのMain/Afterに対応する辞書に表現式を格納します。

・RowGroupKeyの取得

RowGroupKeyノードに値が設定されている場合、その値を取得し記録します。

・RowGroupSizeの取得

RowGroupSizeノードに値が設定されている場合、その値を取得し記録します。
RowGroupSizeノードに値が設定されていない場合、デフォルトのバッチ処理は次の条件で実行されます：

①、データが500MBのメモリを占有した場合（優先判定）
②、上記に該当せず、行数が1万行に達した場合

※あくまで、現状の構造で推測した数値です。
　テスト&運用上で修正する必要かもしれません。

6.2　コールバック駆動のデータ取込・正規化（to Inputスキーマ）

データを行単位で読み込み、検証・正規化後、Inputスキーマ準拠レコードをコールバックで逐次返却する方式を定義します。

プロセスログ

PROCESS タイプ： 対象データファイル「{input_data_file}」の読み込み 実行

リネージの処理対処データソースファイルにより、使うプラグインを選んで処理します。
プラグインの指定があるかどうかを判定し、それに応じてデータファイルのロード処理を行います。

プラグインの指定取得:

「parameter_（ワークフローID）.yaml」ファイルに、リネージ「リネージID」ノードの「ExtendFunctionInfo1」の設定値で判断します。

・プラグインが指定された場合、プラグインの対象データファイルを読み込みます。

下記のデータタイプに対しては、それぞれのデータ形式やレイアウトに応じたプラグインを設計し、
デリゲートコールバック方式でデータレコードを返すようにします。

・Excel
・PDF
・HTML
・Parquet
・TSV/CSV/固定長

←専用
←専用
←専用
←汎用、B145に実装します
←汎用、B145にテキストファイル解析プラグインとして実装します

※上記以外のデータタイプや解析形式を将来的に追加する場合は、プラグインを新規作成するだけで対応可能とします。

振分モジュール経由で、指定されたプラグインを起動します。

引数:

・onDataRow デリゲート
・logUtil LogUtil インスタンス  プラグインにもログ出力するために、追加して
・JSON形式の文字列

処理済みのDataRowを受け取るデリゲート

リネージ関連の各種情報

Copyright(c) 2020 Broadleaf Co.,Ltd.

5/

6

6

14
14

6
6
6
6

14
14

6
6
6
6
6
6
6
6
8
8
8

12
12

14
14

5

6
12

14
14
14
14
14

PTE-010-35.0303-01_プログラム基本設計書

詳細については、「プログラム基本設計書_プラグイン_振分モジュール_解析加工IF定義」を参照してください。

6/

14
14

※JSON文字列には、以下の構造でデータ処理パイプラインの設定情報が含まれています。
ノード名
LineageGroupID
LineageSubGroupId
LineageId
LineageSummary
LineageReousceInfo
TransformyamlInfoDic
DataFileList
MacroVariable
WorkFlowSettings
EventFileContent

説明
リネージグループID
リネージサブグループID
リネージID
リネージ関連情報
リネージリソース情報
入力ファイル情報辞書
入力ファイルリスト
マクロ変数
ワークフロー設定情報
イベントファイル内容

詳細内容
データ処理のリネージグループID
このデータパイプラインの子ワークフローIDです
呼び出し元のリネージ定義 LineageId
リネージのプラグイン、プロセス名称などの設定情報
入出力リソースのエンコーディング、入出力区分、圧縮方式などの設定情報
入力ファイルのrepository、ファイル名称ルール、スキーマyaml名等設定情報
対象ファイルフルパスリスト
マクロ変数定義(マクロ変数名と正則表現)
ログ出力、GCSパス、作業・出力ディレクトリなど、ワークフロー実行に必要な設定
処理実行の契機となるイベント設定。ジョブ情報、通知先メールアドレス、通知API、などを含む。

・プラグインの設定がない場合、

設定不正として、エラーとして、累積ツールを異常終了（255）にします。
エラーログ：

ERROR  タイプ : 「データリネージプロセス管理」で、当リネージID「{0}」の「拡張機能情報1(extend_function_info_1)」が未設定です。

6.3　上記ロードできたデータレコードに対し、データコンバートをしてから、Parquetファイルに出力

メモリコストを控えるために、RowGroupSizeでメモリキャシューをしてから、データコンバート～Parquetファイル出力を行います。

6.3.1　Parquetファイル生成方式の分岐

OutputスキーマyamlファイルのRowGroupKey設定の有無によるParquetファイル生成方式の条件分け

・RowGroupKeyが設定されていない場合、直接Parquetファイルを生成
・RowGroupKeyが設定されている場合、DuckDBを経由してParquetファイルを生成

＞　RowGroupKeyを設定していない場合、直接Parquetファイルに出力

OutputスキーマyamlファイルのRowGroupSizeで、Parquetファイルに出力

1) データコンバート(入力レコード⇒出力レコードの変換)

Inputスキーマカラム操作、転記仕様yamlのマッピング実施、Outputスキーマカラム操作をして

「6.3.2　データコンバート方法」を参照してください。

2) Parquetファイル出力

上記データリスト(RowGroupSize)で、コンバートできたデータをParquetファイルに追加モードで出力します。

「6.3.3　Parquetファイル出力共通設定」を参照してください。

プロセスログ

PROCESS タイプ： データレコードのコンバート完了、出力ファイルに「{table.Count}」件のレコードを追加。

＞　RowGroupKeyを設定している場合、DuckDBにキャシューして、Parquetファイルに出力

DuckDBへのデータ追加のバッチ処理は、次の条件で実行されます：

①、 データが500MBのメモリを達した場合（優先判定）
②、 上記に該当せず、行数が10万行に達した場合

・データコンバート

1) データコンバート(入力レコード⇒出力レコードの変換)

Copyright(c) 2020 Broadleaf Co.,Ltd.

「6.3.2　データコンバート方法」を参照してください。

Inputスキーマカラム操作、転記仕様yamlのマッピング実施、Outputスキーマ操作をして

※あくまで、現状の構造で推測した数値です。
　テスト&運用上で修正する必要かもしれません。

6
6
6
6
6
6
6
6
6
6
6
6

5
9
9
12
14

5

7

7
7

7
7
7

7
7

14
14

7
8
8
8
7

7

7

PTE-010-35.0303-01_プログラム基本設計書

2) コンバートできたデータを DuckDB にインポート（ディスクベース）

対象の コンバートできたデータを DuckDB にインポートし、中間テーブルとして保存します。
メモリ上ではなく、ディスクベースで処理を行うことで、大容量データにも対応可能とします。

プロセスログ

PROCESS タイプ： データレコードのコンバート完了、DuckDBに「{rowCount}」件のレコードを追加。

· Parquetファイル出力

指定RowGroupKeyで、Parquetファイルを生成

Outputスキーマyamlファイルに「RowGroupKey」の指定がある場合だけ、指定キーでもう一回グループして、Parquetファイルの生成を行います。

※データコンバートできたら、更にRowGroupKeyでグループする必要です。

1)　RowGroupKey によるグループ化処理

DuckDB 上のテーブルに対し、RowGroupKeyでデータをグループ化します。
グループ単位で再構成されたデータセットを、SQL を用いて取得・加工します。
※メモリコストを考慮した上で、実装する必要です。

2)　Parquetファイルの生成

＞　RowGroupSizeを設定している場合

RowGroupKey により分割された各グループ内で、 RowGroupSize 件数単位に分割。
分割されたデータごとに、Parquet ファイルに追加モードで出力します。

＞　RowGroupSizeを設定していない場合

RowGroupKey により分割されたデータごとに、Parquet ファイルに追加モードで出力します。

「6.3.3　Parquetファイル出力共通設定」を参照してください。

6.3.1　データコンバート方法

プラグインからonDataRow経由で受け取った各レコードを、指定された転記仕様YAMLに基づき、
リアルタイムでデータコンバートを実施します。

1)Inputスキーマyamlに、カラム表現式にBefore指定あり項目に対して、更にコンバートします。

元データに、ブリ処理をする必要か否かは、カラムの表現式にbefore計算式で記載してるので、

Inputスキーマyamlにカラム表現式に、「Before」変換式があるカラムに対し、指定表現式でコンバートを行う。

＞ Before表現式が設定ありカラム

カラム毎に、Beforeカラム表現式の操作をします。

※表現式の書き方は、下記2種類があります。 詳細は、「プログラム基本設計書_基礎-コンポーネント_ExpressionUtil」に参照

・expr

表現式の実装は自分で実装する必要です。

・expr_js

表現式の実行は、自分で実装する必要なし、規格に則った記述にすれば、JavaScript エンジンがその表現式を解釈し、実行してくれます。
Jint（ジント）は、.NET アプリケーション上で JavaScript を実行するための ECMAScript エンジン です。
C# など .NET 言語から JavaScript を呼び出して実行できる、軽量で高性能な JavaScript インタプリタです。

7/

7
7

14
14

7
7
7
7

7
7
7
7
7
7
7
7
7
7
7
7
7

7

14
14

7
6
6
6
6
6
6
6
6
6
6
6
6
6

2) プリ処理できたレイアウトを出力レイアウトへのマッピングとデータ転送

コンバート処理が完了したデータは、出力レイアウトにマッピングしたら、データを転送する

Outputスキーマyamlファイルに設定された「ITEM_MATCH_MODE」に基づいて、入力解析結果のデータと出力レイアウトの間でカラムマッピング
※詳細は、ルールブックの「2. 入力ファイルと出力ファイルの列マッピングルール（ITEM_MATCH_MODE）」を参照。

Copyright(c) 2020 Broadleaf Co.,Ltd.

7
5
5
5
5
5

PTE-010-35.0303-01_プログラム基本設計書

8/

5
7
6
6
6
6
6

7
7
7
7
7
7
7
7
7
7
7
7
7
7
7

14
14

3) Outputスキーマyamlに、カラム表現式にMain/After指定あり項目に対して、更にコンバートします。

Main/After変換辞書に記録されている各カラムおよび対応する表現式を使って、「Main→After」の順に変換を行う。

＞　Main/After表現式が設定されているカラム

・カラム毎に、Mainカラム表現式の操作をします。
・カラム毎に、Afterカラム表現式の操作をします。

6.3.3　Parquetファイル出力共通設定

Parquetファイルについて、出力方法と圧縮モードもバラメータyamlから取得して、

・出力処理方式（io_div）

GCSストリームにするか、ローカルに出力するかを判断します。
この io_divは、パラメータyamlファイルに、「lineage_info」ノード配下当該リネージIDノード配下の 「out」ノード → 「io_div」ノードに設定しています。

※詳細は、ルールブックのシート「3)DBの基本設定の説明」内の「3-1　入出力区分（io_div）」を参照。

・論理圧縮（compress_method）

Parquetファイルの出力する際の論理圧縮方法を決定します。
論理圧縮の方法は、パラメータyamlファイルに、「lineage_info」ノード配下当該リネージIDノード配下の 「out」ノード→ 「compress_method」ノードに指定してます。

※詳細は、ルールブックのシート「3)DBの基本設定の説明」内の「5-5　圧縮方式(compress_method)」を参照。

7　作成できたParquetファイルの物理圧縮

プロセスログ

PROCESS タイプ： 出力ファイル「{outputFilePath}」の物理圧縮 実行

物理圧縮指定有り場合、共通の圧縮方法を使って、Parquetファイルの圧縮を実施します。

共通「DwhFileProcessUtil」を利用して、出力ファイルを圧縮します。
file:物理圧縮方法

圧縮なしの場合は、noneを設定しますか、空を設定します

設定内容
空白 または file:none
file:zip
file:7z
...

Parquet作成後の物理圧縮方法
圧縮しない
zip
7z
...

※物理圧縮の場合、パスワード指定有りの場合、暗号化かけて、圧縮します。

8　処理完了ログ出力およびDBログ出力

8.1　「データリネージ実行ログ」テーブルに、今回実行の結果をログに出力する。
処理完了の基本情報を「データリネージ実行ログ」テーブルに出力します。
※環境変数からリソース管理DBの接続情報を取得

№ 設定項目
1
2
3
4
5
6 実行順序
プロセスID
7
Copyright(c) 2020 Broadleaf Co.,Ltd.

ログインデックスID
リネージグループID
リネージサブグループID
リネージID
適用バージョン

設定内容
log_index_id
lineage_group_id
lineage_sub_group_id
lineage_id
PGのバージョン番号
lineage_no
PGID

備考
AUTO_INCREMENT
-
-
-
-
-
-

PTE-010-35.0303-01_プログラム基本設計書

8 ワークフローID
9
実行日
10 実行時間
11

実行フェーズ

workflow_name
YYYYMMDD
HHMMSSFFFFFF
end:処理終了

12 実行スタータス

ok:正常終了,warning:警告,error:エラー

-
-
-
-

9/

13

詳細メッセージ

処理結果情報、エラー情報等

14 入力ファイル
15
出力ファイル

作業対象ファイルパス
Parquetファイルパス

複数種類のエラーが検知された場合、
エラーハンドリング設定に従い、ExitCodeが最後のエラー情報を出力する。

※最後のエラー情報とは、以下の2種類のうち、最後に出力された Msg を指します。
  ① 振り分けモジュールで呼び出された Plugin の After 処理の戻り値（Msg）
  ② exe 実行中に発生したエラー情報

11

※512文字まで残って、それ以上、切り捨て

-
-

8.2　ログ出力

転記仕様yamlの「loginfo」の設定により、転記仕様yamlの「logtype」で指定された方法（コンソールログ、ローカルファイル、クロードロギング）を利用してログを出力します

ログタイトル：

2024/02/01 16:53:11 [情報] (NameSpace: ネームスペース WorkFlowName: ワークフロー名 ExecuteTaskId:実行タスクID)

ログ内容：

・基本な設定情報

リネージグループID、リネージID、PGID、execute_task_idなど

・ファイル情報

入出力ファイルパス、途中作成ファイル（パラメータyaml、転記仕様yaml）など情報

・設定情報のチェック結果
設定不正な情報

・開始、終了情報

各処理ステープの開始、終了情報、exit codeなど

logの例：

2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 実行時単位ID(リネージグループID-リネージID) :L00005
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) ツールID :acumulate-file-loader
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) yaml設定内容のチェックが開始しました...
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 転記仕様スキーマ：D:\tmp\transform_spec.yml
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) ツール実行前チェックを行います...
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) yaml設定内容のチェックが正常完了しました。
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 転記仕様にoutfileのfilenameruleの設定が不正ため、下記のinfileに対しますoutfileの名称が重複になります。
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)  ・t001_f0002_3321B0_11-03_001_parts_ver0001.html ⇒ 3321B0_11-03_parts_202311_ver001.Parquet
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)  ・t001_f0002_3321B0_11-03_001_parts_ver0002.html ⇒ 3321B0_11-03_parts_202311_ver001.Parquet
2024/02/01 16:53:11 [警告] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 累積処理が異常終了しました。(警告：99)

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

10/

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書


