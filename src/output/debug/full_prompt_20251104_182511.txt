
# ===================================================

# 【1】レビュー設定 (毎回ユーザーが編集する箇所)

# ===================================================

# 案件固有の情報をここに設定してください。

REVIEW_CONFIG:

# --- 審査対象ファイル ---

今回の設計書: "xxx基本設計書_v2.pdf"

前回の設計書: "xxx基本設計書_v1.pdf"

# その他の参考資料...

ルールブック："ルールブック_v1.pdf"

# --- 招聘する専門家チーム（ペルソナ） ---

# 今回のレビューで有効化したいペルソナの行頭の「#」を削除してください。

ACTIVE_PERSONAS:

- SystemArchitect # 設計思想と一貫性を問う

- QAManager # 品質とテストの観点から曖昧さを問う

# - CSharpTechLead# 実装の実現性とコード品質を問う

# - ProjectManager# スコープ、リスク、見積もりを問う

# - LanguageQualityReviewer # 記述品質、文法を問う

# --- 特別な制約条件 ---

# コスト上限、納期、技術スタックなど、今回のレビューで特に考慮すべき制約を簡潔に記述。

CUSTOM_CONSTRAINTS:

- "クラウド費用は月額XX円以内に収める必要がある。"

- "既存のXXXライブラリとの互換性を維持する必要がある。"

# ===================================================

# 【2】AIへのコア指令 (Core Mission)

# ===================================================

あなたは「基本設計書レビューの専門家」です。

上記の`REVIEW_CONFIG`に基づき、指定された`ACTIVE_PERSONAS`の思考を完全にシミュレートし、多角的な「開発傾向分析レポート」を作成してください。各ペルソナは自身のミッションにのみ集中し、その結果を統合して最終的なレポートを生成します。

# ===================================================

# 【3】ペルソナ・アクティベーション ＆ 指令

# ===================================================

/*

以下のペルソナ定義に基づき、指定されたペルソナを有効化し、それぞれの指令を実行せよ。

*/

---

### **Persona: SystemArchitect (システムアーキテクト)**

#### **思考様式:**

トップダウンで物事を捉える。設計書の一貫性、拡張性、保守性を重視する。「なぜこの設計なのか？」を常に問い、代替案との比較を求める。

#### **指令 (Mission Checklist):**

- **[ ] 論理トレーサビリティの検証:** 要件が「大分類 → 中分類 → 小分類 → 具体的な仕様 → テスト条件」まで、一本の論理的な鎖（ロジックチェーン）で繋がっているか？鎖が切れている箇所を指摘せよ。

- **[ ] 機能要件と非機能要件の分離と結合:**

- **[ ] 技術選定の妥当性評価**

- **[ ] 指摘事項への準拠**

#### ** DeepDive Module [F]: 機能・要件適合性審査**

/* このモジュールが有効化された場合、以下の追加指令を最高優先度で実行せよ */

- **[F1] トレーサビリティマトリクス:** 要求Noごとに、「要求→設計要素（章節,図,表,API等）」のマトリクスを作成せよ。対応箇所がなければ「対応箇所なし」と明記。

- **[F2] 機能分解の完全性:** 各機能の「トリガー条件」「事前/事後条件」「入出力」「主/派生処理」をリストアップし、記述漏れを指摘せよ。

- **[F3] ビジネスルール明確性:** ビジネスルールを「ルールID,内容,例外条件,参照元」で一覧化せよ。必要なら決定表に正規化し、曖昧なルールは「検証不能」とせよ。

- **[F4] 例外時の期待動作:** 各例外発生源に対し、「検知箇所」「対処方法（リトライ,ロールバック等）」「通知方法（エラーコード等）」の記述を明確化せよ。

- **[F5] 非機能の受け皿:** 非機能要件（性能,可用性等）を実現するための具体的なパラメータ（リトライ回数,タイムアウト値等）が設計書に明記されているか評価せよ。

---

#### 🆕【追加】DeepDive Module [R]: 要件適合性・機能整合性審査

*このモジュールが有効化された場合、以下の指令を最高優先度で実行せよ。*

- **[R1] 要件マッピング整合性:**

要件定義書の全要求Noを網羅し、それぞれが設計書内のどの章・どの機能で実現されているかマッピング表を作成せよ。対応箇所がない場合は「未対応」と明記。

- **[R2] 要件→機能→テストの三層トレーサビリティ:**

各要件がどの機能設計・処理仕様・テスト条件に連鎖しているかを検証し、「要件→機能→UTケース」の一貫性を確認。切れている鎖があれば指摘。

- **[R3] 要件の実装可能性:**

要件レベルで曖昧な表現（例：「高速化する」「わかりやすくする」）が設計上どのパラメータ／処理で実現されているかを明確化。不明確な場合は「実装根拠不明」として指摘。

---

### **Persona: QAManager (品質保証マネージャー)**

#### **思考様式:**

ボトムアップで仕様の曖昧さやリスクを探す。テスト担当者が迷わずテストケースを作成できるかを唯一の基準とする。「この記述でテストできるか？」を常に問う。

#### **指令 (Mission Checklist):**

- **[ ] 処理ロジックの具体性評価 (WDT分析):**

- 全ての処理ロジックを「When（条件）」「Do（処理）」「Then（結果）」の観点で分解せよ。

- **特に「Do（処理）」の具体性を厳しく評価せよ。** 「データを処理する」のような曖昧な記述は不適合とする。「どの項目を、どの順序で、どの計算式/ロジックを用いて、どう変換/加工するのか」が、コードを書けるレベルで記述されているか？

- **[ ] 用語と定義の明確化:** 設計書内で使われる専門用語、略語、表の列名などが、一意に解釈できるよう定義されているか？（例：用語集の有無、各表の列定義の明確さ）

- **[ ] 異常系の網羅性:** 正常系だけでなく、想定される異常系（入力エラー、外部APIエラー、タイムアウト、リソース枯渇等）がリストアップされ、それぞれの振る舞いが明確に定義されているか？テストケースに落とし込めるか？

#### **★追加★ DeepDive Module [T]: テスト設計可能性審査**

/* このモジュールが有効化された場合、以下の追加指令を最高優先度で実行せよ */

- **[T1] 因子表（条件×値）:** 各機能のテスト条件（因子）と、その値（水準）を一覧化せよ（同値、境界値、制約等）。

- **[T2] 決定表の網羅性:** ビジネスルールを決定表に変換し、ルールの矛盾や到達不能なルールを指摘せよ。

- **[T3] 組合せ戦略:** 複雑な機能に対し、適切な組合せテスト戦略（ペアワイズ法等）を提案し、テストケース数を見積もれ。

- **[T4] 状態モデル:** 状態を持つ機能の状態遷移図/表を作成し、「状態,イベント,ガード条件,アクション,例外遷移」を網羅せよ。

- **[T5] データ契約の検証可能性:** 入出力データの各フィールドの制約（データ型,桁数,NULL許容,相関制約等）を定義し、サンプルを提示せよ。

- **[T6] インタフェース試験観点:** 各IFについて、「べき等性,再送処理,レート制限,エラーコード体系」等のテスト観点が設計書から読み取れるか評価せよ。

- **[T7] テスト不能箇所:** 現状の設計書でテスト設計が困難な箇所を特定し、最低限必要な「補足情報リスト」を提示せよ。

---

#### 🆕【追加】DeepDive Module [TX]: 要件由来テスト因子・組合せ整合性

*このモジュールが有効化された場合、以下の指令を最高優先度で実行せよ。*

- **[TX1] 要件由来因子抽出:**

要件定義の各項目からテスト設計に必要な入力条件（因子）を抽出し、「要件No／因子名／値（水準）／期待結果」を表形式で整理。

- **[TX2] 組合せ網羅性:**

抽出した因子同士の相互依存関係を分析し、ペアワイズ法または決定表ベースで網羅率を算出。カバレッジが低い場合は「未網羅因子」として指摘。

- **[TX3] 検証不可能因子:**

設計書に記述が不足し、テスト設計上「条件×値」の設定ができない箇所を列挙し、「要件No」「因子」「不足情報」「必要補足」をリストアップ。

---

### **Persona: CSharpTechLead (C#テックリード)**

#### **思考様式:**

実装者の視点で設計書を読む。クリーンなコードが書けるか、パフォーマンスは出るか、.NETのベストプラクティスに沿っているかを重視する。「この設計で美しいコードが書けるか？」を問う。

#### **指令 (Mission Checklist):**

- **[ ] 実装の具体性:** 設計からC#のクラスやメソッドが明確にイメージできるか？特定のライブラリ（例: Entity Framework）の作法に沿っているか？

- **[ ] コード品質リスク:** 設計が複雑な依存関係や低凝集なコンポーネントを生み出さないか？（SOLID原則に反していないか？）

- **[ ] セキュリティとパフォーマンス:** SQLインジェクション等の脆弱性や、ループ処理・データアクセスでの性能ボトルネックに繋がりそうな設計はないか？

---

### ** Persona: ProjectManager (プロジェクトマネージャー)**

#### **思考様式:**

リスク、スコープ、コスト、スケジュールの観点で設計書を読む。タスク分割と工数見積もりが可能か、手戻りリスクはないかを重視する。「この設計で見積もれるか？スコープは守られているか？」を問う。

#### **指令 (Mission Checklist):**

- **[ ] スコープの整合性:** 設計されている全ての機能は、要件定義のスコープ内に収まっているか？オーバースペック（金メッキ）な部分はないか？

- **[ ] 見積もり可能性:** 設計の各項目は、工数見積もりができる粒度で記述されているか？「要調査」「要検討」といった曖昧な記述が残っていないか？

- **[ ] ★追加★ 要求仕様との一致:** 設計内容が`ルールブック`で定義された要求や制約から逸脱していないか？

### **Persona: LanguageQualityReviewer (記述品質レビュアー)**

#### **思考様式:**

文章の正確性・統一性・完全性を最優先に考える。

設計書を「人間およびAIが誤読せず、実装・テストに利用できる日本語仕様書」であるかを検証する。

特に誤字・脱字・文法破綻・表記揺れ・異言語混入を厳密に検出し、文脈的に正しい修正案を提示する。

#### **指令 (Mission Checklist):**

- **[ ] 記述品質チェック（誤字・脱字・用語混在検知）**

- 設計書全体のテキストを解析し、以下のエラーを検出・分類せよ。

- 各検出結果は「箇所・原文・誤り分類・改善案・信頼度(0~1)」で表にまとめること。

- **単純誤字／漏字:**

- 既知単語に類似するが1〜3文字異なる語（例：「プライン」→「プラグイン」）。

- 編集距離 ≤3 または音素一致率 ≥0.8 の語を「誤字疑い」として抽出。

- **動詞欠落・文末欠損:**

- 「〜に」「〜を」「〜へ」などで終わる文を検出し、句末に動詞が存在しない場合、「意味破綻」として報告。

- **混在文字（異言語混入）:**

- 日本語文中に中国語（例：「累计」）、技術用語以外の英語、半角カナ・全角英数字などが混在している箇所を検出。

- **文法破綻・助詞誤用:**

- 不自然な助詞連結（例：「をを」）、述語が欠けている文などを「構文不整合」として報告。

- **用語不一致:**

- 同一概念が複数の表記で出現している場合（例：「Excel／EXCEL／エクセル」）を確認し、正規表記案を提示。

#### ** DeepDive Module [L]: 言語品質・可読性審査**

- **[L1] 文体統一性:**設計書全体（本文、表内テキスト、引用符内のエラーメッセージ、図の注釈などを含む全ての日本語記述）で文体（「です／ます調」と「である／する調」）が統一されているか検証せよ。技術仕様書として「である／する調」への統一を原則とする。

- **[L2] 句読点・助詞チェック:** 「、」の過剰／欠落、「は／が」「を／に」など助詞誤用を検出し、自然な修正案を提示せよ。

- **[L3] 技術用語の正規化:** 業界標準表記（例：Excel、JSON、GCS、リポジトリ、トランザクション）に準拠しているかを確認せよ。異表記を一覧化し、統一案を提示する。

- **[L4] 日本語以外の混在:** 英語・中国語・半角カナなど異言語が混在している箇所を自動検出し、統一表記案を提示せよ。コード例・変数名などは例外とする。

# ===================================================

# 【4】出力形式 (Output Format)

# ===================================================

* **形式:** Googleスプレッドシートにエクスポート可能な、以下の2部構成で表形式で出力すること。

* **原則:

** 第１部 LanguageQualityReviewer (記述品質レビュアー)以外、各ペルソナの指令（Mission Checklist）項目を「要求内容」のベースとし、具体的な指摘事項を記述すること。

| 要求No | 要求内容 (ペルソナ: 指令) | 評価 (〇/△/×) | 適合/不適合箇所 | 適合/不適合理由 | 修正案 (ゴールデンケースを含む) | 対応有無 | 対応方法／非対応理由 |

| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |

| SA-1 | SystemArchitect: 論理トレーサビリティ | △ | 3.1章 機能A | 要件定義書No.123から詳細設計への展開が追跡できない。中分類の設計が欠落している。 | (修正案を具体的に記述) | | |

| QA-1 | QAManager: WDT分析-「Do」の具体性 | × | 4.2.1 データ加工処理 | 処理内容が「マスタを参照して補正」としか書かれておらず、具体的な補正ロジックが不明。 | **【ゴールデンケース】**<br>1. 入力`item_id`をキーに`product_master`を検索。<br>2. `master.price`がNULLでない場合、入力`price`を上書き。<br>3. ...のように、処理手順を番号付きで記述する。 | | |

| ... | ... | ... | ... | ... | ... | | |

** 第2部：LanguageQualityReviewer (記述品質レビュアー)のレビュー結果を、以下の専用テーブル形式で出力する。

| 箇所| 原文| 誤り分類| 改善案| 信頼度(0~1)|



# PDF Content (Markdown format):

# プログラム基本設計書_累積作成ツール (5)_処理詳細_V7

3-1.処理詳細説明(全体)

プログラムID acumulate-file-loader

システム名 f013カタログ部品データ作成
業務機能名 累積作成
プログラム名称 累積EXE

システムID
バージョン
プロジェクトNo

12000599-00

作成日
更新日

2025/2/14
2025/10/30
区分

作成者
更新者

3H.尚坤
3H.尚坤

汎用

詳細内容

・起動パラメータ
コマンド:

acumulate-file-loader.exe -n workflow-namespace -w workflow-name -g lineage-group-id -s lineage-subgroup-id -t execute-task-id -p project-id -i input-path

1/

改訂No. 削

備考
Cloudログ出力用のnamespace
ワークフローの名前
データ処理のリネージID
このデータパイプラインの子ワークフローIDです
今回作業のタスクID
GCPで管理・操作するために、一意の識別子（ユニークID）
処理対象の複数のファイル情報が入っている json
データファイルはトリガーとする場合、非バッチ系、1つファイルの情報
プラグインが処理対象のデータ（ファイル）を特定するための選択子です。

パラメータ:

短縮形式 長い形式

-n
-w
-g
-s
-t
-p
無し
-i
無し

--workflow-namespace
--workflow-name
--lineage-group-id
--lineage-subgroup-id
--execute-task-id
--project-id
--input-list-file
--input-file
--input-file-selector

必須/オプション
必須
必須
必須
必須
オプション
必須
オプション　※1・※2
オプション　※2
オプション　※2

機能
ネームスペース
ワークフロー名
リネージグループID
リネージサブグループID
タスクID
プロジェクトID
処理ユニットのセット(JSONファイル)
データソースファイルパス
処理対象ファイル選択

※1 バッチ系の場合、処理対象は複数件ある場合、そのリストをJSON形のObject Listにして、

別途方法で、分割処理が必要です。

※2 運用上、この3つオプションのパラメータ（--input-list-file、-i/--input-file、--input-file-selector  ）は排他必要です。

・リソース消費

システムリソース負荷に影響する要因

項目
処理データ量
RowGroupSize
データ分布
特殊変換の複雑さ

説明
処理対象のテーブル数、各テーブルの行数・列数により処理量が増加
バッチ処理一度のデータ量（デフォルト1万）で、RowGroupSize が大きいほどメモリ使用量も増えます。
処理対象のファイルが分散配置されている場合、散在すればするほど検索時間が長くなります
特殊な変換処理が多ければ多いほど、処理時間が長くなり、メモリ消費量も増加します

あるサンプルデータでテスト結果 : 量化指標（実測値に基づくリソース消費の定量化）
データ処理量が増えるごとに、CPU使用率も高くなり、メモリ使用量も増加します。

RowGroupSize

特殊変換

10,000

ない

データ件数
500
5,000
1万
10万
100万
200万

CPU使用率
7%
17%
20%
21%
41%
45%

メモリ使用量(MB) ※1
30 MB
97 MB
134 MB
302 MB
1400 MB
2,505 MB

時間(s)
4 s
46 s
74 s
38 s
334 s
1,004 s

※1本処理で使用されるメモリ量は、Workflow の yaml に定義されたテンプレートのリソース制限（resources.limits.memory など）に依存します。

設定された上限を超えた場合、メモリオーバーフロー（OOM）が発生し、Pod が強制終了される可能性があります。

・処理詳細

1　ツール起動

上記「起動パラメータ」仕様を参照して起動します。

＞　必要の引数が未設定の場合、

コンソールエラーメッセージ「必須オプション「xxxx」が未設定です。W/F動作時は定義YAMLテンプレートで引数に指定してください。」を表示して、ツール起動に失敗

> 排他の引数(-i/--input-file と --input-list-file と --input-file-selector)が同時設定の場合、

コンソールエラーメッセージ「オプション 「-i/--input-file」, 「--input-list-file」, 「--input-file-selector」は同時に指定できません。どちらか一方のみを指定してください。」を表示して、ツール起動に失敗

13
13
13

13
13
13

4
4
4
5
4
4
4
4

4
4
4
4
4
4
4

12

12

2.　ツール初期処理

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

2.1　バラメータyaml設定内容の読込・チェック、リネージ情報の取得

リネージグループ、サブグループ、ワークフローなどを初期化します。

リネージグループID、リネージサブグループIDで、バラメータyamlファイル名を決めて、
・
・

「ワークフローID」　: 「リネージグループID + リネージサブグループID」
「parameter_(ワークフローID).yaml」

ツール実行に必要なリネージ設定情報、各種パラメータを、事前に作成されたパラメータyamlファイルから、リネージの対応情報をリードします。

・パラメータyamlファイル(parameter_(ワークフローID).yaml)の存在チェック

> パラメータyaml(parameter_(ワークフローID).yaml) が見つかった場合、処理用のリネージ情報を取得します。

ツール実行用のリネージ設定内容をバラメータyamlから取得しておいて

ツール実行に必要なリネージIDを取得します。(バラメータyamlに、ワークフローのリネージサブグループにあるリネージ設定は全て入ってる)
・すべてのLineage情報の取得および設定パラメータの取得

パラメータyamlファイルを読込み、「ParameterInfoEntity」にキャシューします。

> パラメータyaml(parameter_(ワークフローID).yaml) が見つからない場合、

エラーメッセージ「指定されたパラメータファイル「xxxx」が見つかりません。」を表示して、処理中断します。
※ログ出力の設定すらもないなので、コンソール画面にこのログを出力します。

2.2　ログ初期化

ログ出力の初期化

初期化バラメータ：
project-id
execute_task_id
process_id
workflow-name
name-space
logfilepath
logtypelist
loglevel

GCPリソースを管理・操作する際に使用する、一意の識別子（ユニークID）
実行タスクID
プロセス識別用の一意なID
現在のワークフローの名称
ログ出力用のnamespace
ログ出力ファイルのパス
ログ出力先設定リスト（console, local, cloudlogging）
ログレベル（release, debug）

「workflow_config_common.yml」ファイルに設定できて、LOG出力は下記の3つ設定可能です。

-console
-local
-cloud_logging

コンソールにログを出力します。
ローカルファイルにログを保存します。
CloudLogging 上にログを出力します。

2.3 リネージの処理開始ログ、「データリネージ実行ログ」に開始ログの出力

ツールが処理対象の設定情報(リネージ)を取得するために、リネージID（lineage_id）が必要です。
ツールプロセスID(processId)、引数のタスクID(task_id)を使います。

＞　リネージID（lineage_id）が1件のみ取得できた場合

処理開始ログの出力

処理の開始時に、リネージ名（process_name）を使用して、以下の形式でログを出力する：

INFO : 「{process_name}」 を開始します...

「データリネージ実行ログ」への開始ログ出力

本リネージ処理の開始情報を「データリネージ実行ログ」テーブルに出力します。

※環境変数からリソース管理DBの接続情報を取得します。

設定項目

設定内容

№

1

2

3

4

ログインデックスID
リネージグループID
リネージサブグループID
リネージID

Copyright(c) 2020 Broadleaf Co.,Ltd.

log_index_id
lineage_group_id
lineage_sub_group_id
lineage_id

備考
AUTO_INCREMENT
-
-
-

2/

6
6
6
6
6
6
6
6
6
6
6
12
6
6

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

PTE-010-35.0303-01_プログラム基本設計書

適用バージョン

5
6 実行順序
プロセスID
7
8 ワークフローID
実行日
9
10 実行時間
11
12 実行スタータス
詳細メッセージ
13
14 入力ファイル
出力ファイル
15

実行フェーズ

PGのバージョン番号
lineage_no
PGID
workflow_name
YYYYMMDD
HHMMSSFFFFFF
start:処理開始
info:情報
処理開始提示メッセージ
作業対象ファイルパス
空

-
-
-
-
-
-
処理開始ステータス
-
-
-
-

＞　上記以外の場合

＞　リネージID（lineage_id） が1件も取得できなかった場合、エラーログを以下の形式で出力します。

エラーログ

ERROR : パラメータYAML「{parameterYamlFile}」で、プロセスID「{processId}」・タスクID「{taskId}」に該当するリネージ定義は見つかりません。

＞　リネージID（lineage_id） が複数件取得された場合、エラーログを以下の形式で出力します。

エラーログ

ERROR : パラメータYAML「{parameterYamlFile}」で、プロセスID「{processId}」とタスクID「{taskId}」のリネージ定義が重複しています。

上記いずれのケースでも、「データリネージ実行ログ」に異常開始ログを出力します。

※環境変数からリソース管理DBの接続情報を取得します。

№

設定項目

設定内容

1

3

2

ログインデックスID
リネージグループID
リネージサブグループID
リネージID
4
5 適用バージョン
6 実行順序
プロセスID
7
8 ワークフローID
9 実行日
10 実行時間
11
12 実行スタータス
13 詳細メッセージ
14 入力ファイル
出力ファイル
15

実行フェーズ

log_index_id
lineage_group_id
lineage_sub_group_id
空
空
空
PGID
workflow_name
YYYYMMDD
HHMMSSFFFFFF
start:処理開始
error：エラー
空
作業対象ファイルパス
空

備考
AUTO_INCREMENT
-
-
リネージIDの取得に失敗したため、空とします
リネージID関連項目が取得できなかったため、空とします
リネージID関連項目が取得できなかったため、空とします
-
-
-
-
処理開始ステータス
-
-
-
-

3/

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

3 リネージIDに基づいて転記仕様yamlファイルの確定と読み込み

 指定されたリネージIDに基づき、対応する転記仕様YAMLファイル（transform_spec.(リネージID).yaml）を特定し、パースします。
※　ワークフローで使用する転記仕様 yaml ファイルは、Argo Workflows の artifacts 機能により、事前に共有・参照可能な状態となっています。

プロセスログ

PROCESS タイプ： 転記仕様のYAMLファイル「{transSpecYamlPath}」の読み込み 実行

転記仕様yamlファイル（transform_spec_(リネージID).yaml）の内容を読み取っておいて

・転記仕様yamlファイルが見つかりました、

転記仕様yaml「transform_spec_(リネージID).yaml」を見て、このリネージで指定した転記情報を取得します。

・ 転記仕様yamlファイルが見つからないと、処理中断します。

エラーログ

Copyright(c) 2020 Broadleaf Co.,Ltd.

ERROR タイプ: 転記仕様ファイル「{transSpecYamlPath}」が見つかりません。

7
7
14
14

PTE-010-35.0303-01_プログラム基本設計書

4　入力、出力データファイルのレイアウト定義用スキーマyamlファイルのダウンロード

Input、Outputスキーマyamlファイルをダウンロードします。

パラメータyaml、転記仕様yamlから操作対象ファイルの定義情報を抽出し、指定GCSから、当該スキーマyamlファイルをダウンロードします。

プロセスログ

PROCESS タイプ： 入力・出力ファイルのスキーマ YAML ファイルのダウンロード 実行

まず、各スキーマyamlファイルの保存箇所を取得して、

バラメータyamlファイル(parameter_(ワークフローID).yaml)から、下記の情報を取得します：

・bucket: スキーマyamlファイルが保存されているGCSのバケット名。
ノード「setting_info」のサブノード「GCS」のサブノード「bucket」

・schemayamlpath: スキーマyamlファイルのパス。

ノード「setting_info」のサブノード「GCS」のサブノード「schemayamlpath」

 次に、対象のスキーマyamlファイル名を取得して

転記仕様yamlファイル(transform_spec_(リネージID).yaml)にから、下記の情報を取得します：

・Inputスキーマyamlファイル

ノード「input_file」の全てのサブノード「schemafilepath」

・Outputスキーマyamlファイル

ノード「output_file」の全てのサブノード「schemafilepath」

DownloadSchemaFile
パラメータ：

・転記仕様yaml
・ワークフロー設定情報

機能：

lineage で使用される可能性のある schema yaml ファイルのダウンロード

取得した bucket、schemayamlpath と Inputスキーマyamlファイル名、Outputスキーマyamlファイル名 を利用して、GCSから対象のスキーマyamlファイルをダウンロードします。

5　処理対象リスト作成

起動パラメータ（-i/--input-file 、 --input-list-file 、--input-file-selector ）で指定された処理対象に対して、
本リネージの設定に基づきグループ化を行い、処理を実行します。

> パラメータ (-i/--input-file) だけを設定した場合

入力ファイル名が命名規則に従っているかのチェック

＞ 入力ファイル名が命名規則に従っていない場合：

エラーログを出力する（ExitCode：244）

ERROR タイプ： 対象データファイル「{excelFilePath}」は、当リネージの命名規則「{fileNameRule}」に適合しません。

以下の処理を実行する：

・ 単一ファイルを対象リストに格納します。

> パラメータ (input-file-selector ) だけを設定した場合

以下の処理を実行する：

・ 処理対象を対象リストに格納します。

> パラメータ (--input-list-file) だけを設定した場合

以下の処理を実行する：

· JSON ファイルを読み込み、対象リストに格納します。

> -i/--input-file、--input-list-file、および --input-file-selector のいずれも設定されていない場合

以下の処理を実行する：

・ 「repository 」と「filenamerule」に基づき、対象ファイルリストに格納します。

6　対象データファイルから、データをリードして、データコンバートを実行し、Parquetファイルを作成します。

データリード、データコンバート、Parquetファイルに書きこみの流れで実施します。

Copyright(c) 2020 Broadleaf Co.,Ltd.

6.1 Inputスキーマ、Outスキーマyamlファイルのリード

4/

14
14

5
5

5
5

4
4
3
3

PTE-010-35.0303-01_プログラム基本設計書

6

コンバート処理開始時に、InputスキーマyamlファイルおよびOutputスキーマyamlファイルを解析し、必要な情報を取得します。

① Inputスキーマyamlファイルの解析

プロセスログ

PROCESS タイプ： 入力スキーマ YAML「{SchemaFilePath}」ファイルを解析し、標準転記の表現式を取得します...

・Before表現式の取得

special_transformノードに値が設定されている場合、そのカラムのBeforeに対応する辞書に表現式を格納します。

② Outputスキーマyamlファイルの解析

プロセスログ

PROCESS タイプ： 出力スキーマ YAML「{SchemaFilePath}」ファイルを解析し、標準転記の表現式を取得します...

・Main/After表現式の取得

special_transformノードに値が設定されている場合、そのカラムのMain/Afterに対応する辞書に表現式を格納します。

・RowGroupKeyの取得

RowGroupKeyノードに値が設定されている場合、その値を取得し記録します。

・RowGroupSizeの取得

RowGroupSizeノードに値が設定されている場合、その値を取得し記録します。
RowGroupSizeノードに値が設定されていない場合、デフォルトのバッチ処理は次の条件で実行されます：

①、データが500MBのメモリを占有した場合（優先判定）
②、上記に該当せず、行数が1万行に達した場合

※あくまで、現状の構造で推測した数値です。
　テスト&運用上で修正する必要かもしれません。

6.2　コールバック駆動のデータ取込・正規化（to Inputスキーマ）

データを行単位で読み込み、検証・正規化後、Inputスキーマ準拠レコードをコールバックで逐次返却する方式を定義します。

プロセスログ

PROCESS タイプ： 対象データファイル「{input_data_file}」の読み込み 実行

リネージの処理対処データソースファイルにより、使うプラグインを選んで処理します。
プラグインの指定があるかどうかを判定し、それに応じてデータファイルのロード処理を行います。

プラグインの指定取得:

「parameter_（ワークフローID）.yaml」ファイルに、リネージ「リネージID」ノードの「ExtendFunctionInfo1」の設定値で判断します。

・プラグインが指定された場合、プラグインの対象データファイルを読み込みます。

下記のデータタイプに対しては、それぞれのデータ形式やレイアウトに応じたプラグインを設計し、
デリゲートコールバック方式でデータレコードを返すようにします。

・Excel
・PDF
・HTML
・Parquet
・TSV/CSV/固定長

←専用
←専用
←専用
←汎用、B145に実装します
←汎用、B145にテキストファイル解析プラグインとして実装します

※上記以外のデータタイプや解析形式を将来的に追加する場合は、プラグインを新規作成するだけで対応可能とします。

振分モジュール経由で、指定されたプラグインを起動します。

引数:

・onDataRow デリゲート
・logUtil LogUtil インスタンス  プラグインにもログ出力するために、追加して
・JSON形式の文字列

処理済みのDataRowを受け取るデリゲート

リネージ関連の各種情報

Copyright(c) 2020 Broadleaf Co.,Ltd.

5/

6

6

14
14

6
6
6
6

14
14

6
6
6
6
6
6
6
6
8
8
8

12
12

14
14

5

6
12

14
14
14
14
14

PTE-010-35.0303-01_プログラム基本設計書

詳細については、「プログラム基本設計書_プラグイン_振分モジュール_解析加工IF定義」を参照してください。

6/

14
14

※JSON文字列には、以下の構造でデータ処理パイプラインの設定情報が含まれています。
ノード名
LineageGroupID
LineageSubGroupId
LineageId
LineageSummary
LineageReousceInfo
TransformyamlInfoDic
DataFileList
MacroVariable
WorkFlowSettings
EventFileContent

説明
リネージグループID
リネージサブグループID
リネージID
リネージ関連情報
リネージリソース情報
入力ファイル情報辞書
入力ファイルリスト
マクロ変数
ワークフロー設定情報
イベントファイル内容

詳細内容
データ処理のリネージグループID
このデータパイプラインの子ワークフローIDです
呼び出し元のリネージ定義 LineageId
リネージのプラグイン、プロセス名称などの設定情報
入出力リソースのエンコーディング、入出力区分、圧縮方式などの設定情報
入力ファイルのrepository、ファイル名称ルール、スキーマyaml名等設定情報
対象ファイルフルパスリスト
マクロ変数定義(マクロ変数名と正則表現)
ログ出力、GCSパス、作業・出力ディレクトリなど、ワークフロー実行に必要な設定
処理実行の契機となるイベント設定。ジョブ情報、通知先メールアドレス、通知API、などを含む。

・プラグインの設定がない場合、

設定不正として、エラーとして、累積ツールを異常終了（255）にします。
エラーログ：

ERROR  タイプ : 「データリネージプロセス管理」で、当リネージID「{0}」の「拡張機能情報1(extend_function_info_1)」が未設定です。

6.3　上記ロードできたデータレコードに対し、データコンバートをしてから、Parquetファイルに出力

メモリコストを控えるために、RowGroupSizeでメモリキャシューをしてから、データコンバート～Parquetファイル出力を行います。

6.3.1　Parquetファイル生成方式の分岐

OutputスキーマyamlファイルのRowGroupKey設定の有無によるParquetファイル生成方式の条件分け

・RowGroupKeyが設定されていない場合、直接Parquetファイルを生成
・RowGroupKeyが設定されている場合、DuckDBを経由してParquetファイルを生成

＞　RowGroupKeyを設定していない場合、直接Parquetファイルに出力

OutputスキーマyamlファイルのRowGroupSizeで、Parquetファイルに出力

1) データコンバート(入力レコード⇒出力レコードの変換)

Inputスキーマカラム操作、転記仕様yamlのマッピング実施、Outputスキーマカラム操作をして

「6.3.2　データコンバート方法」を参照してください。

2) Parquetファイル出力

上記データリスト(RowGroupSize)で、コンバートできたデータをParquetファイルに追加モードで出力します。

「6.3.3　Parquetファイル出力共通設定」を参照してください。

プロセスログ

PROCESS タイプ： データレコードのコンバート完了、出力ファイルに「{table.Count}」件のレコードを追加。

＞　RowGroupKeyを設定している場合、DuckDBにキャシューして、Parquetファイルに出力

DuckDBへのデータ追加のバッチ処理は、次の条件で実行されます：

①、 データが500MBのメモリを達した場合（優先判定）
②、 上記に該当せず、行数が10万行に達した場合

・データコンバート

1) データコンバート(入力レコード⇒出力レコードの変換)

Copyright(c) 2020 Broadleaf Co.,Ltd.

「6.3.2　データコンバート方法」を参照してください。

Inputスキーマカラム操作、転記仕様yamlのマッピング実施、Outputスキーマ操作をして

※あくまで、現状の構造で推測した数値です。
　テスト&運用上で修正する必要かもしれません。

6
6
6
6
6
6
6
6
6
6
6
6

5
9
9
12
14

5

7

7
7

7
7
7

7
7

14
14

7
8
8
8
7

7

7

PTE-010-35.0303-01_プログラム基本設計書

2) コンバートできたデータを DuckDB にインポート（ディスクベース）

対象の コンバートできたデータを DuckDB にインポートし、中間テーブルとして保存します。
メモリ上ではなく、ディスクベースで処理を行うことで、大容量データにも対応可能とします。

プロセスログ

PROCESS タイプ： データレコードのコンバート完了、DuckDBに「{rowCount}」件のレコードを追加。

· Parquetファイル出力

指定RowGroupKeyで、Parquetファイルを生成

Outputスキーマyamlファイルに「RowGroupKey」の指定がある場合だけ、指定キーでもう一回グループして、Parquetファイルの生成を行います。

※データコンバートできたら、更にRowGroupKeyでグループする必要です。

1)　RowGroupKey によるグループ化処理

DuckDB 上のテーブルに対し、RowGroupKeyでデータをグループ化します。
グループ単位で再構成されたデータセットを、SQL を用いて取得・加工します。
※メモリコストを考慮した上で、実装する必要です。

2)　Parquetファイルの生成

＞　RowGroupSizeを設定している場合

RowGroupKey により分割された各グループ内で、 RowGroupSize 件数単位に分割。
分割されたデータごとに、Parquet ファイルに追加モードで出力します。

＞　RowGroupSizeを設定していない場合

RowGroupKey により分割されたデータごとに、Parquet ファイルに追加モードで出力します。

「6.3.3　Parquetファイル出力共通設定」を参照してください。

6.3.1　データコンバート方法

プラグインからonDataRow経由で受け取った各レコードを、指定された転記仕様YAMLに基づき、
リアルタイムでデータコンバートを実施します。

1)Inputスキーマyamlに、カラム表現式にBefore指定あり項目に対して、更にコンバートします。

元データに、ブリ処理をする必要か否かは、カラムの表現式にbefore計算式で記載してるので、

Inputスキーマyamlにカラム表現式に、「Before」変換式があるカラムに対し、指定表現式でコンバートを行う。

＞ Before表現式が設定ありカラム

カラム毎に、Beforeカラム表現式の操作をします。

※表現式の書き方は、下記2種類があります。 詳細は、「プログラム基本設計書_基礎-コンポーネント_ExpressionUtil」に参照

・expr

表現式の実装は自分で実装する必要です。

・expr_js

表現式の実行は、自分で実装する必要なし、規格に則った記述にすれば、JavaScript エンジンがその表現式を解釈し、実行してくれます。
Jint（ジント）は、.NET アプリケーション上で JavaScript を実行するための ECMAScript エンジン です。
C# など .NET 言語から JavaScript を呼び出して実行できる、軽量で高性能な JavaScript インタプリタです。

7/

7
7

14
14

7
7
7
7

7
7
7
7
7
7
7
7
7
7
7
7
7

7

14
14

7
6
6
6
6
6
6
6
6
6
6
6
6
6

2) プリ処理できたレイアウトを出力レイアウトへのマッピングとデータ転送

コンバート処理が完了したデータは、出力レイアウトにマッピングしたら、データを転送する

Outputスキーマyamlファイルに設定された「ITEM_MATCH_MODE」に基づいて、入力解析結果のデータと出力レイアウトの間でカラムマッピング
※詳細は、ルールブックの「2. 入力ファイルと出力ファイルの列マッピングルール（ITEM_MATCH_MODE）」を参照。

Copyright(c) 2020 Broadleaf Co.,Ltd.

7
5
5
5
5
5

PTE-010-35.0303-01_プログラム基本設計書

8/

5
7
6
6
6
6
6

7
7
7
7
7
7
7
7
7
7
7
7
7
7
7

14
14

3) Outputスキーマyamlに、カラム表現式にMain/After指定あり項目に対して、更にコンバートします。

Main/After変換辞書に記録されている各カラムおよび対応する表現式を使って、「Main→After」の順に変換を行う。

＞　Main/After表現式が設定されているカラム

・カラム毎に、Mainカラム表現式の操作をします。
・カラム毎に、Afterカラム表現式の操作をします。

6.3.3　Parquetファイル出力共通設定

Parquetファイルについて、出力方法と圧縮モードもバラメータyamlから取得して、

・出力処理方式（io_div）

GCSストリームにするか、ローカルに出力するかを判断します。
この io_divは、パラメータyamlファイルに、「lineage_info」ノード配下当該リネージIDノード配下の 「out」ノード → 「io_div」ノードに設定しています。

※詳細は、ルールブックのシート「3)DBの基本設定の説明」内の「3-1　入出力区分（io_div）」を参照。

・論理圧縮（compress_method）

Parquetファイルの出力する際の論理圧縮方法を決定します。
論理圧縮の方法は、パラメータyamlファイルに、「lineage_info」ノード配下当該リネージIDノード配下の 「out」ノード→ 「compress_method」ノードに指定してます。

※詳細は、ルールブックのシート「3)DBの基本設定の説明」内の「5-5　圧縮方式(compress_method)」を参照。

7　作成できたParquetファイルの物理圧縮

プロセスログ

PROCESS タイプ： 出力ファイル「{outputFilePath}」の物理圧縮 実行

物理圧縮指定有り場合、共通の圧縮方法を使って、Parquetファイルの圧縮を実施します。

共通「DwhFileProcessUtil」を利用して、出力ファイルを圧縮します。
file:物理圧縮方法

圧縮なしの場合は、noneを設定しますか、空を設定します

設定内容
空白 または file:none
file:zip
file:7z
...

Parquet作成後の物理圧縮方法
圧縮しない
zip
7z
...

※物理圧縮の場合、パスワード指定有りの場合、暗号化かけて、圧縮します。

8　処理完了ログ出力およびDBログ出力

8.1　「データリネージ実行ログ」テーブルに、今回実行の結果をログに出力する。
処理完了の基本情報を「データリネージ実行ログ」テーブルに出力します。
※環境変数からリソース管理DBの接続情報を取得

№ 設定項目
1
2
3
4
5
6 実行順序
プロセスID
7
Copyright(c) 2020 Broadleaf Co.,Ltd.

ログインデックスID
リネージグループID
リネージサブグループID
リネージID
適用バージョン

設定内容
log_index_id
lineage_group_id
lineage_sub_group_id
lineage_id
PGのバージョン番号
lineage_no
PGID

備考
AUTO_INCREMENT
-
-
-
-
-
-

PTE-010-35.0303-01_プログラム基本設計書

8 ワークフローID
9
実行日
10 実行時間
11

実行フェーズ

workflow_name
YYYYMMDD
HHMMSSFFFFFF
end:処理終了

12 実行スタータス

ok:正常終了,warning:警告,error:エラー

-
-
-
-

9/

13

詳細メッセージ

処理結果情報、エラー情報等

14 入力ファイル
15
出力ファイル

作業対象ファイルパス
Parquetファイルパス

複数種類のエラーが検知された場合、
エラーハンドリング設定に従い、ExitCodeが最後のエラー情報を出力する。

※最後のエラー情報とは、以下の2種類のうち、最後に出力された Msg を指します。
  ① 振り分けモジュールで呼び出された Plugin の After 処理の戻り値（Msg）
  ② exe 実行中に発生したエラー情報

11

※512文字まで残って、それ以上、切り捨て

-
-

8.2　ログ出力

転記仕様yamlの「loginfo」の設定により、転記仕様yamlの「logtype」で指定された方法（コンソールログ、ローカルファイル、クロードロギング）を利用してログを出力します

ログタイトル：

2024/02/01 16:53:11 [情報] (NameSpace: ネームスペース WorkFlowName: ワークフロー名 ExecuteTaskId:実行タスクID)

ログ内容：

・基本な設定情報

リネージグループID、リネージID、PGID、execute_task_idなど

・ファイル情報

入出力ファイルパス、途中作成ファイル（パラメータyaml、転記仕様yaml）など情報

・設定情報のチェック結果
設定不正な情報

・開始、終了情報

各処理ステープの開始、終了情報、exit codeなど

logの例：

2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 実行時単位ID(リネージグループID-リネージID) :L00005
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) ツールID :acumulate-file-loader
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) yaml設定内容のチェックが開始しました...
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 転記仕様スキーマ：D:\tmp\transform_spec.yml
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) ツール実行前チェックを行います...
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) yaml設定内容のチェックが正常完了しました。
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 転記仕様にoutfileのfilenameruleの設定が不正ため、下記のinfileに対しますoutfileの名称が重複になります。
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)  ・t001_f0002_3321B0_11-03_001_parts_ver0001.html ⇒ 3321B0_11-03_parts_202311_ver001.Parquet
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)  ・t001_f0002_3321B0_11-03_001_parts_ver0002.html ⇒ 3321B0_11-03_parts_202311_ver001.Parquet
2024/02/01 16:53:11 [警告] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 累積処理が異常終了しました。(警告：99)

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

10/

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書



---

# プログラム基本設計書_累積作成ツール (5)_処理詳細_V6

3-1.処理詳細説明(全体)

プログラムID acumulate-file-loader

システム名 f013カタログ部品データ作成
業務機能名 累積作成
プログラム名称 累積EXE

システムID
バージョン
プロジェクトNo

12000599-00

作成日
更新日

2025/2/14
2025/10/24
区分

作成者
更新者

3H.尚坤
3H.尚坤

汎用

詳細内容

・起動パラメータ
コマンド:

acumulate-file-loader.exe -n workflow-namespace -w workflow-name -g lineage-group-id -s lineage-subgroup-id -t task-id -p project-id -i input-path

備考
Cloudログ出力用のnamespace
ワークフローの名前
データ処理のリネージID
このデータパイプラインの子ワークフローIDです
今回作業のタスクID
GCPで管理・操作するために、一意の識別子（ユニークID）
処理対象の複数のファイル情報が入っている json
データファイルはトリガーとする場合、非バッチ系、1つファイルの情報
プラグインが処理対象のデータ（ファイル）を特定するための選択子です。

パラメータ:

短縮形式 長い形式
-n
-w
-g
-s
-t
-p
無し
-i
無し

--workflow-namespace
--workflow-name
--lineage-group-id
--lineage-subgroup-id
--task-id
--project-id
--input-list-file
--input-file
--input-file-selector

必須/オプション
必須
必須
必須
必須
オプション
必須
オプション　※1・※2
オプション　※2
オプション　※2

機能
ネームスペース
ワークフロー名
リネージグループID
リネージサブグループID
タスクID
プロジェクトID
処理ユニットのセット(JSONファイル)
データソースファイルパス
処理対象ファイル選択

※1 バッチ系の場合、処理対象は複数件ある場合、そのリストをJSON形のObject Listにして、

別途方法で、分割処理が必要です。

※2 運用上、この3つオプションのパラメータ（--input-list-file、-i/--input-file、--input-file-selector  ）は排他必要です。

・リソース消費

システムリソース負荷に影響する要因

項目
処理データ量
RowGroupSize
データ分布
特殊変換の複雑さ

説明
処理対象のテーブル数、各テーブルの行数・列数により処理量が増加
バッチ処理一度のデータ量（デフォルト1万）で、RowGroupSize が大きいほどメモリ使用量も増えます。
処理対象のファイルが分散配置されている場合、散在すればするほど検索時間が長くなります
特殊な変換処理が多ければ多いほど、処理時間が長くなり、メモリ消費量も増加します

あるサンプルデータでテスト結果 : 量化指標（実測値に基づくリソース消費の定量化）

データ処理量が増えるごとに、CPU使用率も高くなり、メモリ使用量も増加します。

RowGroupSize

特殊変換

10,000

ない

データ件数
500
5,000
1万
10万
100万
200万

CPU使用率 メモリ使用量(MB) ※1
30 MB
7%
17%
97 MB
134 MB
20%
302 MB
21%
1400 MB
41%
2,505 MB
45%

時間(s)
4 s
46 s
74 s
38 s
334 s
1,004 s

※1本処理で使用されるメモリ量は、Workflow の yaml に定義されたテンプレートのリソース制限（resources.limits.memory など）に依存します。

設定された上限を超えた場合、メモリオーバーフロー（OOM）が発生し、Pod が強制終了される可能性があります。

・処理詳細

1　ツール起動

上記「起動パラメータ」仕様を参照して起動します。

＞　必要の引数が未設定の場合、

コンソールエラーメッセージ「必須オプション「xxxx」が未設定です。W/F動作時は定義YAMLテンプレートで引数に指定してください。」を表示して、ツール起動に失敗

> 排他の引数(-i/--input-file と --input-list-file と --input-file-selector)が同時設定の場合、

コンソールエラーメッセージ「オプション '-i/--input-file', '--input-list-file', '--input-file-selector' は同時に指定できません。どちらか一方のみを指定してください。」を表示して、ツール起動に失敗

2.　ツール初期処理

リネージグループ、サブグループ、ワークフローなどを初期化します。

リネージグループID、リネージサブグループIDで、バラメータyamlファイル名を決めて、

1/

改訂No. 削

13
13
13

13
13
13

4
4
4
5
4
4
4
4
4

4
4
4
4
4
4
4

12

12

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

・
・

「ワークフローID」　: 「リネージグループID + リネージサブグループID」
「parameter_(ワークフローID).yaml」

2.1　バラメータyaml設定内容の読込・チェック、リネージ情報の取得

ツール実行に必要なリネージ設定情報、各種パラメータを、事前に作成されたパラメータyamlファイルから、リネージの対応情報をリードします。

・パラメータyamlファイル(parameter_(ワークフローID).yaml)の存在チェック

> パラメータyaml(parameter_(ワークフローID).yaml) が見つかった場合、処理用のリネージ情報を取得します。

ツール実行用のリネージ設定内容をバラメータyamlから取得しておいて

ツール実行に必要なリネージIDを取得します。(バラメータyamlに、ワークフローのリネージサブグループにあるリネージ設定は全て入ってる)
・すべてのLineage情報の取得および設定パラメータの取得

パラメータyamlファイルを読込み、「ParameterInfoEntity」にキャシューします。

・処理リネージ(処理対象)の設定情報取得

処理リネージ(処理対象)の設定情報を取得するために、「リネージID(lineage_id)」が必要で、
ツールプロセスID、引数のタスクID(task_id)を使います。

> パラメータyaml(parameter_(ワークフローID).yaml) が見つからない場合、

エラーメッセージ「指定されたパラメータファイル「xxxx」が見つかりません。」を表示して、処理中断します。
※ログ出力の設定すらもないなので、コンソール画面にこのログを出力します。

2.2　ログ初期化、処理開始ログ出力およびDBログ出力

2.2.1 ログ初期化

共通の「LogUtility」を使用し、ログ出力に関する設定を初期化する。
あわせて、累積ツール固有のメッセージ定義ファイルを読み込み、ログメッセージのロードを行う。

パラメータ説明
パラメータ
LogUtilityConfig の構成

LogUtilityConfig の構成
項目
ProjectId
ExecuteTaskId
ProcessId
WorkFlowName
NameSpace
LocalLogFilePath
LogLevel
LogTypeList
ErrorHanding
MsgTemplatePathList

内容
ログ設定情報を含むLogUtilityConfig オブジェクト（下記を参照）

内容
GCPリソースを管理・操作する際に使用する、一意の識別子（ユニークID）
実行タスクID
プロセス識別用の一意なID
現在実行中のワークフロー名
ログ出力用のnamespace
ログファイルの出力パス
ログレベル（release, debug）
ログ出力先リスト（console, local, cloudlogging）
エラーハンドリングポリシー（例: resume, skip, terminate）
メッセージテンプレートファイルのパス

詳細は、LogUtilityのInitializer（LogUtilityConfig config)を参照する。

2.2.2 処理開始ログ出力

処理開始時に、開始ログおよびデータリネージの実行開始ログ（DBログ）を出力します。

出力内容
msgId
CI0001_AppStarted

メッセージテンプレート
「{0}」を開始します...

出力先：

説明
{0}: プログラム名(Process_Name)またはモジュール名

① 「データリネージ実行ログ」テーブル
② 「workflow_config.yml」ファイルに設定できて、LOG出力先は下記の3つ設定可能です。
コンソールにログを出力します。

-console

2/

6
6
6
6
6
6
6
6
6
6
6
8
8

6
12
6
6

13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

-local
-cloud_logging

ローカルファイルにログを保存します。
CloudLogging 上にログを出力します。

詳細は、LogUtilityのInitLog(ExecuteLogEntity? executeLogEntity, string msgId, List<string> paramList)を参照する。

3 リネージIDに基づいて転記仕様yamlファイルの確定と読み込み

 指定されたリネージIDに基づき、対応する転記仕様YAMLファイル（transform_spec.(リネージID).yaml）を特定し、パースします。
※　ワークフローで使用する転記仕様 yaml ファイルは、Argo Workflows の artifacts 機能により、事前に共有・参照可能な状態となっています。

転記仕様yamlファイル（transform_spec_(リネージID).yaml）の内容を読み取っておいて

・転記仕様yamlファイルが見つかりました、

転記仕様yaml「transform_spec_(リネージID).yaml」を見て、このリネージで指定した転記情報を取得します。

・ 転記仕様yamlファイルが見つからないと

「転記仕様ファイル「xxxx」が見つかりません」とエラーメッセージを表示して、処理中断します。

4　入力、出力データファイルのレイアウト定義用スキーマyamlファイルのダウンロード

Input、Outputスキーマyamlファイルをダウンロードします。

パラメータyaml、転記仕様yamlから操作対象ファイルの定義情報を抽出し、指定GCSから、当該スキーマyamlファイルをダウンロードします。

まず、各スキーマyamlファイルの保存箇所を取得して、

バラメータyamlファイル(parameter_(ワークフローID).yaml)から、下記の情報を取得します：

・bucket: スキーマyamlファイルが保存されているGCSのバケット名。
ノード「setting_info」のサブノード「GCS」のサブノード「bucket」

・schemayamlpath: スキーマyamlファイルのパス。

ノード「setting_info」のサブノード「GCS」のサブノード「schemayamlpath」

 次に、対象のスキーマyamlファイル名を取得して

転記仕様yamlファイル(transform_spec_(リネージID).yaml)にから、下記の情報を取得します：

・Inputスキーマyamlファイル

ノード「input_file」の全てのサブノード「schemafilepath」

・Outputスキーマyamlファイル

ノード「output_file」の全てのサブノード「schemafilepath」

取得した bucket、schemayamlpath と Inputスキーマyamlファイル名、Outputスキーマyamlファイル名 を利用して、GCSから対象のスキーマyamlファイルをダウンロードします。

5　処理対象リスト作成

起動パラメータ（-i/--input-file または --process-unit-list-file）で指定された処理対象に対して、
本リネージの設定に基づきグループ化を行い、処理を実行します。

> パラメータ (-i/--input-file) だけを設定した場合

以下の処理を実行する：

・ 単一ファイルを対象リストに格納します。

> パラメータ (input-file-selector ) だけを設定した場合

以下の処理を実行する：

・ 処理対象を対象リストに格納します。

> パラメータ (--input-list-file) だけを設定した場合

以下の処理を実行する：

· JSON ファイルを読み込み、対象リストに格納します。

> -i/--input-file、--input-list-file、および input-file-selector のいずれも設定されていない場合

以下の処理を実行する：

・ 「repository 」と「filenamerule」に基づき、対象ファイルリストに格納します。

6　対象データファイルから、データをリードして、データコンバートを実行し、Parquetファイルを作成します。

データリード、データコンバート、Parquetファイルに書きこみの流れで実施します。

DownloadSchemaFile
パラメータ：

・転記仕様yaml
・ワークフロー設定情報

機能：

lineage で使用される可能性のある schema yaml ファイルのダウンロード

3/

13
13

13

8
8
7
7
7
7
7
7
7
12

5
5

5
5

4
4
3
3

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

6.1 Inputスキーマ、Outスキーマyamlファイルのリード

コンバート処理開始時に、InputスキーマyamlファイルおよびOutputスキーマyamlファイルを解析し、必要な情報を取得します。

① Inputスキーマyamlファイルの解析

・Before表現式の取得

special_transformノードに値が設定されている場合、そのカラムのBeforeに対応する辞書に表現式を格納します。

② Outputスキーマyamlファイルの解析

・Main/After表現式の取得

special_transformノードに値が設定されている場合、そのカラムのMain/Afterに対応する辞書に表現式を格納します。

・RowGroupKeyの取得

RowGroupKeyノードに値が設定されている場合、その値を取得し記録します。

・RowGroupSizeの取得

RowGroupSizeノードに値が設定されている場合、その値を取得し記録します。
RowGroupSizeノードに値が設定されていない場合、デフォルトのバッチ処理は次の条件で実行されます：

①、データが500MBのメモリを占有した場合（優先判定）
②、上記に該当せず、行数が1万行に達した場合

6.2　コールバック駆動のデータ取込・正規化（to Inputスキーマ）

データを行単位で読み込み、検証・正規化後、Inputスキーマ準拠レコードをコールバックで逐次返却する方式を定義します。

リネージの処理対処データソースファイルにより、使うプラグインを選んで処理します。
専用プラグインの指定があるかどうかを判定し、それに応じてデータファイルのロード処理を行います。

専用プラグインの指定取得:

「parameter_（ワークフローID）.yaml」ファイルに、リネージ「リネージID」ノードの「ExtendFunctionInfo1」の設定値で判断します。

・プラグインが指定された場合、プラグインの対象データファイルを読み込みます。

プラグインはデリゲートコールバック方式で動作するため、
データ読込む側では、以下のデリゲートを1つ提供する必要があります。

・onDataRow：処理済みのDataRowを受け取るデリゲート

詳細については、「プログラム基本設計書_プラグイン_振分モジュール_解析加工IF定義」を参照してください。

下記のデータタイプに対しては、それぞれのデータ形式やレイアウトに応じた専用プラグインを設計し、
デリゲートコールバック方式でデータを返すようにします。

・Excel
・PDF
・HTML
・Parquet
・TSV/CSV/固定長

←専用
←専用
←専用
←汎用、B145に実装します
←汎用、B145にテキストファイル解析プラグインとして実装します

※上記以外のデータタイプや解析形式を将来的に追加する場合は、専用プラグインを新規作成するだけで対応可能とします。

※あくまで、現状の構造で推測した数値です。
　テスト&運用上で修正する必要かもしれません。

振分モジュールでは、指定された専用プラグインを起動するため、以下の引数を指定します：

・onDataRowデリゲート

・JSON形式の構造体

※JSON文字列には、以下の構造でデータ処理パイプラインの設定情報が含まれています。
ノード名
LineageGroupID
LineageSubGroupId
LineageId
LineageSummary
LineageReousceInfo

説明
リネージグループID

リネージサブグループID
リネージID
リネージ関連情報
リネージリソース情報

詳細内容
データ処理のリネージグループID
このデータパイプラインの子ワークフローIDです
呼び出し元のリネージ定義 LineageId
リネージのプラグイン、プロセス名称などの設定情報
入出力リソースのエンコーディング、入出力区分、圧縮方式などの設定情報

4/

6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
8
8
8

12
12

5

13
13
13
13

6
12

13
13

13
6
6
6
6
6
6
6

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

TransformyamlInfoDic
DataFileList
MacroVariable
WorkFlowSettings
EventFileContent

入力ファイル情報辞書
入力ファイルリスト
マクロ変数
ワークフロー設定情報
イベントファイル内容

入力ファイルのrepository、ファイル名称ルール、スキーマyaml名等設定情報
対象ファイルフルパスリスト
マクロ変数定義(マクロ変数名と正則表現)
ログ出力、GCSパス、作業・出力ディレクトリなど、ワークフロー実行に必要な設定
処理実行の契機となるイベント設定。ジョブ情報、通知先メールアドレス、通知API、などを含む。

・プラグインの設定がない場合、

設定不正として、エラーとして、累積ツールを異常終了（255）にします。
エラーログ：

「データリネージプロセス管理」で、当リネージID「{LineageID}」の「拡張機能情報1(extend_function_info_1)」が未設定です。

6.3　上記ロードできたデータレコードに対し、データコンバートをしてから、Parquetファイルに出力

メモリコストを控えるために、RowGroupSizeでメモリキャシューをしてから、データコンバート～Parquetファイル出力を行います。

6.3.1　Parquetファイル生成方式の分岐

OutputスキーマyamlファイルのRowGroupKey設定の有無によるParquetファイル生成方式の条件分け

・RowGroupKeyが設定されていない場合、直接Parquetファイルを生成
・RowGroupKeyが設定されている場合、DuckDBを経由してParquetファイルを生成

＞　RowGroupKeyを設定していない場合、直接Parquetファイルに出力

OutputスキーマyamlファイルのRowGroupSizeで、Parquetファイルに出力

1) データコンバート(入力レコード⇒出力レコードの変換)

Inputスキーマカラム操作、転記仕様yamlのマッピング実施、Outputスキーマカラム操作をして

「6.3.2　データコンバート方法」を参照してください。

2) Parquetファイル出力

上記データリスト(RowGroupSize)で、コンバートできたデータをParquetファイルに追加モードで出力します。

「6.3.3　Parquetファイル出力共通設定」を参照してください。

＞　RowGroupKeyを設定している場合、DuckDBにキャシューして、Parquetファイルに出力

DuckDBへのデータ追加のバッチ処理は、次の条件で実行されます：

①、 データが500MBのメモリを達した場合（優先判定）
②、 上記に該当せず、行数が10万行に達した場合

・データコンバート

1) データコンバート(入力レコード⇒出力レコードの変換)

Inputスキーマカラム操作、転記仕様yamlのマッピング実施、Outputスキーマ操作をして

「6.3.2　データコンバート方法」を参照してください。

2) コンバートできたデータを DuckDB にインポート（ディスクベース）

対象の コンバートできたデータを DuckDB にインポートし、中間テーブルとして保存します。
メモリ上ではなく、ディスクベースで処理を行うことで、大容量データにも対応可能とします。

· Parquetファイル出力

指定RowGroupKeyで、Parquetファイルを生成

Outputスキーマyamlファイルに「RowGroupKey」の指定がある場合だけ、指定キーでもう一回グループして、Parquetファイルの生成を行います。

※データコンバートできたら、更にRowGroupKeyでグループする必要です。

1)　RowGroupKey によるグループ化処理

DuckDB 上のテーブルに対し、RowGroupKeyでデータをグループ化します。
グループ単位で再構成されたデータセットを、SQL を用いて取得・加工します。
※メモリコストを考慮した上で、実装する必要です。

2)　Parquetファイルの生成

＞　RowGroupSizeを設定している場合

※あくまで、現状の構造で推測した数値です。
　テスト&運用上で修正する必要かもしれません。

5/

6
6
6
6
6

5
9
9
12

5

7

7
7

7
7
7
7
7
7
7
8
8
8
7

7
7

7
7

7
7
7
7
7
7
7
7
7
7
7
7
7

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

RowGroupKey により分割された各グループ内で、 RowGroupSize 件数単位に分割。
分割されたデータごとに、Parquet ファイルに追加モードで出力します。

＞　RowGroupSizeを設定していない場合

RowGroupKey により分割されたデータごとに、Parquet ファイルに追加モードで出力します。

「6.3.3　Parquetファイル出力共通設定」を参照してください。

6.3.2　データコンバート方法

プラグインからonDataRow経由で受け取った各レコードを、指定された転記仕様YAMLに基づき、
リアルタイムでデータコンバートを実施します。

1)Inputスキーマyamlに、カラム表現式にBefore指定あり項目に対して、更にコンバートします。

元データに、ブリ処理をする必要か否かは、カラムの表現式にbefore計算式で記載してるので、

Inputスキーマyamlにカラム表現式に、「Before」変換式があるカラムに対し、指定表現式でコンバートを行う。

＞ Before表現式が設定ありカラム

カラム毎に、Beforeカラム表現式の操作をします。

※表現式の書き方は、下記2種類があります。 詳細は、「プログラム基本設計書_基礎-コンポーネント_ExpressionUtil」に参照

・expr

・expr_js

表現式の実装は自分で実装する必要です。

表現式の実行は、自分で実装する必要なし、規格に則った記述にすれば、JavaScript エンジンがその表現式を解釈し、実行してくれます。
Jint（ジント）は、.NET アプリケーション上で JavaScript を実行するための ECMAScript エンジン です。
C# など .NET 言語から JavaScript を呼び出して実行できる、軽量で高性能な JavaScript インタプリタです。

2) プリ処理できたレイアウトを出力レイアウトへのマッピングとデータ転送

コンバート処理が完了したデータは、出力レイアウトにマッピングしたら、データを転送する

Outputスキーマyamlファイルに設定された「ITEM_MATCH_MODE」に基づいて、入力解析結果のデータと出力レイアウトの間でカラムマッピング
※詳細は、ルールブックの「2. 入力ファイルと出力ファイルの列マッピングルール（ITEM_MATCH_MODE）」を参照。

3) Outputスキーマyamlに、カラム表現式にMain/After指定あり項目に対して、更にコンバートします。

Main/After変換辞書に記録されている各カラムおよび対応する表現式を使って、「Main→After」の順に変換を行う。

＞　Main/After表現式が設定されているカラム

・カラム毎に、Mainカラム表現式の操作をします。
・カラム毎に、Afterカラム表現式の操作をします。

6.3.3　Parquetファイル出力共通設定

Parquetファイルについて、出力方法と圧縮モードもバラメータyamlから取得して、

・出力処理方式（io_div）

GCSストリームにするか、ローカルに出力するかを判断します。
この io_divは、パラメータyamlファイルに、「lineage_info」ノード配下当該リネージIDノード配下の 「out」ノード → 「io_div」ノードに設定しています。

※詳細は、ルールブックのシート「3)DBの基本設定の説明」内の「3-1　入出力区分（io_div）」を参照。

・論理圧縮（compress_method）

Parquetファイルの出力する際の論理圧縮方法を決定します。
論理圧縮の方法は、パラメータyamlファイルに、「lineage_info」ノード配下当該リネージIDノード配下の 「out」ノード→ 「compress_method」ノードに指定してます。

※詳細は、ルールブックのシート「3)DBの基本設定の説明」内の「5-5　圧縮方式(compress_method)」を参照。

7　作成できたParquetファイルの物理圧縮

物理圧縮指定有り場合、共通の圧縮方法を使って、Parquetファイルの圧縮を実施します。

共通「DwhFileProcessUtil」を利用して、出力ファイルを圧縮します。

6/

7
7
7
7
7
7

7

13
13

7
6
6
6
6
6
6
6
6
6
6
6
6
6

7
5
5
5
5
5
5
7
6
6
6
6
6
6

7
7
7
7
7
7
7
7
7
7
7
7
7
7
7

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

file:物理圧縮方法

圧縮なしの場合は、noneを設定しますか、空を設定します

設定内容
空白 または file:none
file:zip
file:7z
...

Parquet作成後の物理圧縮方法
圧縮しない
zip
7z
...

※物理圧縮の場合、パスワード指定有りの場合、暗号化かけて、圧縮します。

8　ログ出力（処理完了ログ／DB書込みログ）

処理の開始・終了・主要ステップ通過を標準化して記録し、「いつ／何の処理が／どの入力に対して／どう結果になったか」を追跡可能にします。
それで、異常時には、参照すべきログと特定手順が一意に分かることを目指すために、

8.1 処理完了ログ

処理完了時に、処理完了ログおよびデータリネージ実行完了ログ（DBログ）を出力します。

出力内容
msgId
区分
正常完了 CI0006_AppSuccess

メッセージテンプレート
「{0}」が正常完了しました。

異常終了 CI0007_AppFailure

「{0}」が異常終了しました。エラー内容: {1}

説明
{0}: プログラム名(Process_Name)またはモジュール名

{0}: プログラム名(Process_Name)またはモジュール名

{1}: 例外メッセージ（Exception.Message）

詳細は、LogUtilityのFinalLog(ExecuteLogEntity? executeLogEntity, string msgId, List<string> paramList, ExitCodes exitCode)を参照する。

8.2 logの例

ログタイトル：

2024/02/01 16:53:11 [情報] (NameSpace: ネームスペース WorkFlowName: ワークフロー名 ExecuteTaskId:実行タスクID)

ログ内容：

・基本な設定情報

リネージグループID、リネージID、PGID、execute_task_idなど

・ファイル情報

入出力ファイルパス、途中作成ファイル（パラメータyaml、転記仕様yaml）など情報

・設定情報のチェック結果
設定不正な情報

・開始、終了情報

各処理ステープの開始、終了情報、exit codeなど

例：

2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 実行時単位ID(リネージグループID-リネージID) :L00005
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) ツールID :acumulate-file-loader
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) yaml設定内容のチェックを開始します...
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 転記仕様スキーマ：D:\tmp\transform_spec.yml
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) ツール実行前チェックを行います...
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)
2024/02/01 16:53:11 [情報] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) yaml設定内容のチェックが正常完了しました。
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 転記仕様にoutfileのfilenameruleの設定が不正ため、下記のinfileに対しますoutfileの名称が重複になります。
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)  ・t001_f0002_3321B0_11-03_001_parts_ver0001.html ⇒ 3321B0_11-03_parts_202311_ver001.Parquet
2024/02/01 16:53:11 [エラー] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:)  ・t001_f0002_3321B0_11-03_001_parts_ver0002.html ⇒ 3321B0_11-03_parts_202311_ver001.Parquet
2024/02/01 16:53:11 [警告] (NameSpace: market-vehicle-data WorkFlowName: workflow-lg0013-130001-chlc9 ExecuteTaskId:) 累積処理が異常終了しました。(警告：99)

· 異常処理:

※詳細については、ルールブックのシート「2)WF定義」内の「8 DWH案件のエラー制御」をご参照ください。

7/

12
14
14
14
14
14
14

14

14

14

12

11

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書

8/

Copyright(c) 2020 Broadleaf Co.,Ltd.

PTE-010-35.0303-01_プログラム基本設計書



---

# ⚠️ 重要な出力制約 ⚠️

**必ず以下のルールを守ること：**

1. **完全性**: PDFに含まれるすべての要求項目を、**1行も漏らさずに**出力すること
2. **省略禁止**: 「以下省略」「...」などで途中で切ってはいけない
3. **全行出力**: 最初の行から最後の行まで、すべての行を含めること
4. **トークン制限**: 出力トークン上限は65536まで使用可能。長くても問題ない

**出力行数が6行以下の場合、確実に抽出漏れがあるため、再度全体を確認して完全に出力すること。**
